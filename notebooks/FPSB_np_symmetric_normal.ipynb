{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n Player FPSB Auction with symmetric valuation distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as ut\n",
    "from torch.optim.optimizer import Optimizer, required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnelearn.strategy import NeuralNetStrategy, TruthfulStrategy\n",
    "from bnelearn.bidder import Bidder\n",
    "from bnelearn.mechanism import FirstPriceSealedBidAuction, VickreyAuction\n",
    "from bnelearn.optimizer import ES\n",
    "from bnelearn.environment import AuctionEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "#\n",
    "#plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "# Use specific cuda gpu if desired \n",
    "#(i.e. for running multiple experiments in parallel)\n",
    "specific_gpu = 3\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The following cell fully defines an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = 'fpsb/10p/normal/symmetric'\n",
    "run_name = '3'\n",
    "logdir = os.path.join(root_path, 'notebooks', run_dir , run_name)\n",
    "\n",
    "## Experiment setup\n",
    "n_players = 10\n",
    "n_items = 1\n",
    "\n",
    "# valuation distribution\n",
    "valuation_mean = 10.0\n",
    "valuation_std = 5.0\n",
    "\n",
    "def strat_to_bidder(strategy, batch_size):\n",
    "    return Bidder.normal(valuation_mean, valuation_std, strategy, batch_size = batch_size)\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 2**17\n",
    "input_length = 1\n",
    "\n",
    "# strategy model architecture\n",
    "size_hidden_layer = 10\n",
    "\n",
    "# optimization params\n",
    "epoch = 10000\n",
    "learning_rate = 3e-1\n",
    "lr_decay = True\n",
    "lr_decay_every = 1000\n",
    "lr_decay_factor = 0.7\n",
    "baseline = True\n",
    "momentum = 0.5\n",
    "\n",
    "sigma = .05 #ES noise parameter\n",
    "n_perturbations = 256\n",
    "\n",
    "# plot and log training options\n",
    "plot_epoch = 250 #plot and log optima this often\n",
    "calculate_optima = False #whether to log stats regarding optimum (expensive - time)\n",
    "write_graph = True # whether to log graph to disk\n",
    "plot_points = min(100, batch_size)\n",
    "plot_xmin = int(max(0, valuation_mean - 3*valuation_std))\n",
    "plot_xmax = int(valuation_mean + 3*valuation_std)\n",
    "\n",
    "plot_ymin = 0\n",
    "plot_ymax = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Bid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Menezes et al. 2005., the optimal bid for symmetric valuations $v$ that are distributed with cdf $F(v)$ for $n$ players in this setting is given by\n",
    "\n",
    "$$b^*(v) = v - \\frac{\\int_0^v F(x)^{n-1} dx}{F(v)^{n-1}} $$\n",
    "\n",
    "\n",
    "We implement it here for calculating comparison metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "\n",
    "common_dist = torch.distributions.normal.Normal(loc = valuation_mean, scale = valuation_std)\n",
    "\n",
    "# TODO: investigate where everything is allocated. possibly move GPU vectors to CPU completely instead of shuffling around for integration?\n",
    "def optimal_bid(valuation: torch.Tensor or np.ndarray or float) -> torch.Tensor:\n",
    "    \n",
    "    # For float and numpy --> convert to tensor\n",
    "    if not isinstance(valuation, torch.Tensor):\n",
    "        valuation = torch.tensor(valuation, dtype = torch.float)           \n",
    "    # For float / 0d tensors --> unsqueeze to allow list comprehension below\n",
    "    if valuation.dim() == 0:\n",
    "        valuation.unsqueeze_(0)\n",
    "    \n",
    "    # shorthand notation for F^(n-1)\n",
    "    Fpowered = lambda v: torch.pow(common_dist.cdf(v), n_players - 1)  \n",
    "    \n",
    "    # do the calculations\n",
    "    numerator = torch.tensor(\n",
    "            [integrate.quad(Fpowered, 0, v)[0] for v in valuation],\n",
    "            device = valuation.device\n",
    "        ).reshape(valuation.shape)                                 \n",
    "    return valuation - numerator / Fpowered(valuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyperparams(writer, e):\n",
    "    writer.add_scalar('hyperparams/batch_size', batch_size, e)\n",
    "    writer.add_scalar('hyperparams/size_hidden_layer', size_hidden_layer, 0)\n",
    "    writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "    writer.add_scalar('hyperparams/momentum', momentum, e)\n",
    "    writer.add_scalar('hyperparams/sigma', sigma, e)\n",
    "    writer.add_scalar('hyperparams/n_perturbations', n_perturbations, e)\n",
    "   \n",
    "    \n",
    "# predefine points for plotting optimal curve to save cpu-bound integrations\n",
    "v_opt = np.linspace(plot_xmin, plot_xmax, 100) # 100 points more than enough\n",
    "b_opt = optimal_bid(v_opt).numpy()\n",
    "\n",
    "def plot_bid_function(fig, v,b, writer=None, e=None, plot_points=plot_points):\n",
    "    \n",
    "    # subsample points and plot\n",
    "    v = v.detach().cpu().numpy()[:plot_points]\n",
    "    b= b.detach().cpu().numpy()[:plot_points]\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    plt.cla()\n",
    "    plt.xlim(plot_xmin, plot_xmax)\n",
    "    plt.ylim(plot_ymin, plot_ymax)\n",
    "    plt.plot(v,b, 'o', v_opt, b_opt, 'r--')\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    if writer:\n",
    "        writer.add_figure('eval/bid_function', fig, e)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot optimal bid to ensure appropriate boundaries have been chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bid_function(None, torch.tensor([0]),torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model.\n",
    "We'll ensure the initialization provides positive outputs on the domain we are interested in, as otherwise we can't learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_is_positive = False\n",
    "while not output_is_positive:\n",
    "    model = NeuralNetStrategy(input_length,\n",
    "                              size_hidden_layer = size_hidden_layer,\n",
    "                              requires_grad=False\n",
    "                             ).to(device)\n",
    "    \n",
    "    if model(torch.tensor([float(valuation_mean)], device=device)) > 0:\n",
    "        output_is_positive = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism = FirstPriceSealedBidAuction(cuda = True)\n",
    "env = AuctionEnvironment(mechanism,\n",
    "                  agents = [], #dynamically built\n",
    "                  max_env_size = n_players - 1, #\n",
    "                  batch_size = batch_size,\n",
    "                  n_players =n_players,\n",
    "                  strategy_to_bidder_closure = strat_to_bidder\n",
    "                 )\n",
    "optimizer = ES(model=model, environment = env,\n",
    "               lr = learning_rate, momentum=momentum,\n",
    "               sigma=sigma, n_perturbations=n_perturbations,\n",
    "               baseline=baseline, env_type='dynamic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(logdir, flush_secs=120) as writer:\n",
    "    overhead_mins = 0\n",
    "    torch.cuda.empty_cache()\n",
    "    log_hyperparams(writer, 0)\n",
    "    fig = plt.figure()\n",
    "    for e in range(epoch+1):\n",
    "        # lr decay?\n",
    "        if lr_decay and e % lr_decay_every == 0 and e > 0:\n",
    "            learning_rate = learning_rate * lr_decay_factor\n",
    "            log_hyperparams(writer, e)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "\n",
    "        # always: do optimizer step\n",
    "        utility = -optimizer.step()\n",
    "        writer.add_scalar('eval/utility', utility, e)\n",
    "\n",
    "        if e % plot_epoch == 0:\n",
    "            start_time = timer()\n",
    "            # log statistics and plot current function output\n",
    "            bidder = strat_to_bidder(model, batch_size)\n",
    "            bidder.draw_valuations_()\n",
    "            v = bidder.valuations\n",
    "            b = bidder.get_action()\n",
    "            # calculate stats regarding optimal behaviour (expensive for general valuation priors!)\n",
    "            if calculate_optima:\n",
    "                op_e = optimal_bid(v)\n",
    "                share = b.mean()/op_e.mean()\n",
    "                diff = (b-op_e).mean()\n",
    "                writer.add_scalar('eval/share', share, e)\n",
    "                writer.add_scalar('eval/diff', diff, e) \n",
    "                print(\"Epoch {}: \\ttotal share: {:.3f}, diff: {:.3f}, \\tutility: {:.3f}\".format(e, share, diff, utility))\n",
    "            else:\n",
    "                print(\"Epoch {}: \\ttotal share: {}, diff: {}, \\tutility: {:.3f}\".format(e, '?', '?', utility))\n",
    "                \n",
    "                \n",
    "            if write_graph:\n",
    "                writer.add_graph(model, bidder.valuations)            \n",
    "                    \n",
    "            \n",
    "            plot_bid_function(fig, v,b,writer,e)\n",
    "            \n",
    "            elapsed = timer() - start_time\n",
    "            overhead_mins = overhead_mins + elapsed/60\n",
    "            writer.add_scalar('eval/overhead_mins', overhead_mins, e)\n",
    "            print(\"Logging checkpoint took {:.2f}s.\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
