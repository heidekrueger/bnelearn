{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    \n",
    "import torch\n",
    "from bnelearn.mechanism import RockPaperScissors, PrisonersDilemma, MatchingPennies, BattleOfTheSexes, JordanGame\n",
    "from bnelearn.environment import MatrixGameEnvironment\n",
    "from bnelearn.bidder import MatrixGamePlayer\n",
    "from bnelearn.strategy import MatrixGameStrategy,FictitiousPlayStrategy, FictitiousPlaySmoothStrategy, FictitiousPlayMixedStrategy\n",
    "from bnelearn.optimizer import ES\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "specific_gpu = 5\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = [\"FPM\",\"RPS\"]\n",
    "param_tau = [0.00001,10,0.99] #[0.,10,0.9]\n",
    "initial_beliefs = None# torch.Tensor([[500,500],[500,500]]).to(device) #initial_beliefs = torch.Tensor([[59.5,40.5],[40.5,59.5]]).to(device)\n",
    "\n",
    "options = {\"FP\": FictitiousPlayStrategy,\n",
    "           \"FPS\": FictitiousPlaySmoothStrategy,\n",
    "           \"FPM\": FictitiousPlayMixedStrategy,\n",
    "           \"PD\": PrisonersDilemma,\n",
    "           \"MP\": MatchingPennies,\n",
    "           \"BoS\": BattleOfTheSexes,\n",
    "           \"JG\": JordanGame,\n",
    "          \"RPS\": RockPaperScissors}\n",
    "\n",
    "run_name = time.strftime('{}_%Y-%m-%d %a %H:%M:%S'.format(setting[0]))\n",
    "game_name = setting[1]\n",
    "logdir = os.path.join(root_path, 'notebooks', 'matrix', game_name, run_name)\n",
    "logdir\n",
    "\n",
    "## Experiment setup\n",
    "epoch = 500000\n",
    "\n",
    "## Environment settings\n",
    "#Dummies here\n",
    "batch_size = 1\n",
    "input_length = 1\n",
    "\n",
    "# optimization params\n",
    "'''\n",
    "All with 10000 epoch\n",
    "PD:\n",
    "FP [] - Converges to 0 quickly\n",
    "FPS [0.5, 10, 0.99]: - Converges to [0.12,0.88]\n",
    "FPS [0.0 ,10, 0.90] - Converges to 0 quickly\n",
    "FPM [0.5, 10, 0.99] - Converges to [0.27,0.73]\n",
    "FPM [0.0, 10, 0.90] - Converges to [0.27,0.73]\n",
    "\n",
    "MP:\n",
    "FP [] -\n",
    "FPS [0.5, 10, 0.99] - Converges to\n",
    "FPS [0.0 ,10, 0.90] - Converges to\n",
    "FPM [0.5, 10, 0.99] - Converges to\n",
    "FPM [0.0, 10, 0.90] - Converges to\n",
    "\n",
    "BoS:\n",
    "FP -\n",
    "FPS - When tau_minimum too small (<0.5) and updates too extreme (<0.9) and too often (<10), FPS escapes MNE and runs to PNE\n",
    "FPS [0.5,10,0.99] - Sometimes find MNE with initial_beliefs = torch.Tensor([[600,400],[400,600]]).to(device))\n",
    "FPM - Find MNE with FPM: parameters: 0., 10, 0.9 and initial_beliefs = torch.Tensor([[6,4],[4,6]]).to(device))\n",
    "FPM - TODO: Check why Equilibrium is [0.55,0.45] here.\n",
    "\n",
    "Jordan Game:\n",
    "FP - Cycles but historical distribution nicely converges\n",
    "FPS [0, 10, 0.9] - Cycles with:\n",
    "FPS [0.5,10,0.99] - Converges to [0.5,0.5,0.5]\n",
    "FPM - Converges at [0.5,0.5,0.5] TODO: Is this equilibrium for all?\n",
    "'''\n",
    "tau_minimum = param_tau[0]\n",
    "tau_update_interval =  param_tau[1]\n",
    "tau_update =  param_tau[2]\n",
    "\n",
    "param = \"tau_minimum: {} \\ntau_update_interval: {} \\ntau_update: {}\".format(tau_minimum,tau_update_interval,tau_update)\n",
    "\n",
    "\n",
    "specific_gpu = 5\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "# Wrapper transforming a strategy to bidder, used by the optimizer\n",
    "# this is a dummy, valuation doesn't matter\n",
    "def strat_to_player(strategy, batch_size, player_position=None):\n",
    "    return MatrixGamePlayer(strategy, batch_size = batch_size, player_position=player_position)\n",
    "\n",
    "\n",
    "game = options[setting[1]]()\n",
    "# init strategies\n",
    "strats = [None] * game.n_players\n",
    "players = [None] * game.n_players\n",
    "if setting[0] == \"FP\" or setting[0] == \"FPS\":\n",
    "    for i in range(game.n_players):\n",
    "        strats[i] = options[setting[0]](game = game, initial_beliefs = initial_beliefs)\n",
    "else:\n",
    "    strat0 = options[setting[0]](game = game, initial_beliefs = initial_beliefs)\n",
    "    for i in range(game.n_players):\n",
    "        strats[i] = strat0   \n",
    "\n",
    "# init players\n",
    "for i in range(game.n_players):\n",
    "    players[i] = strat_to_player(strats[i], batch_size = batch_size, player_position = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel updating\n",
    "print(param)\n",
    "with SummaryWriter(log_dir=logdir, flush_secs=30) as writer:\n",
    "    writer.add_text('hyperparams/hyperparameter', param, 0)\n",
    "    # Log init_beliefs for replicability\n",
    "    for i,strat in enumerate(strats):\n",
    "        writer.add_text('strategy {}/init_beliefs'.format(i), str(strat.historical_actions), 0)\n",
    "    torch.cuda.empty_cache()\n",
    "    for e in range(epoch):\n",
    "        actions = [None] * len(players)\n",
    "        for i,playr in enumerate(players):\n",
    "            actions[i] = playr.get_action()\n",
    "\n",
    "        if e%1000 == 0:\n",
    "            print(actions)\n",
    "\n",
    "        for _,strategy in enumerate(strats):\n",
    "            strategy.update_observations(actions)\n",
    "            strategy.update_beliefs()\n",
    "            if ((setting[0] == \"FPS\" or setting[0] == \"FPM\") and\n",
    "                e > 0 and e%tau_update_interval == 0 and strategy.tau >= tau_minimum):\n",
    "                strategy.update_tau(tau_update)\n",
    "\n",
    "        # Logging\n",
    "        for i,playr in enumerate(players):\n",
    "            # Historical probability for actions\n",
    "            writer.add_histogram('eval/p{}_action_distribution'.format(i), actions[i].view(-1).cpu().numpy(), e)\n",
    "            for a in range(len(playr.strategy.probs[i])-1):\n",
    "                # Historical probability for actions\n",
    "                writer.add_scalar('eval_player_{}/hist_prob_action_{}'.format(i,a), playr.strategy.probs[i][a], e)\n",
    "                # Current period actions \n",
    "                if setting[0] == \"FPM\":\n",
    "                    writer.add_scalar('eval_player_{}/prob_action_{}'.format(i,a), actions[i][a], e)\n",
    "                else:\n",
    "                    writer.add_scalar('eval_player_{}/prob_action_{}'.format(i,a), playr.strategy.probs_self[a], e)\n",
    "            \n",
    "            # Expected Utility\n",
    "            if setting[0] == \"FPM\":\n",
    "                writer.add_scalar('eval_player_{}/exp_utility'.format(i), (playr.strategy.exp_util[i] * actions[i]).sum(), e)\n",
    "            else:\n",
    "                writer.add_scalar('eval_player_{}/exp_utility'.format(i), playr.strategy.exp_util[actions[i]], e)\n",
    "\n",
    "        # Actual Utility\n",
    "        _actions = [torch.zeros(strats[0].n_actions[i], dtype = torch.float, device = game.device)\n",
    "                      for i in range(strats[0].n_players)\n",
    "                     ]\n",
    "        for player,action in enumerate(actions):\n",
    "            if setting[0] == \"FPM\":\n",
    "                _actions[player] = action\n",
    "            else:\n",
    "                _actions[player][action] = 1\n",
    "        for i in range(len(players)):\n",
    "            if setting[0] == \"FPM\":\n",
    "                writer.add_scalar('eval_player_{}/utility'.format(i), (game.calculate_expected_action_payoffs(_actions, i) * actions[i]).sum(), e)\n",
    "            else:\n",
    "                writer.add_scalar('eval_player_{}/utility'.format(i), (game.calculate_expected_action_payoffs(_actions, i)[actions[i]]), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
