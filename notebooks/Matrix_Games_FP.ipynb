{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    \n",
    "import torch\n",
    "from bnelearn.mechanism import RockPaperScissors, PrisonersDilemma, MatchingPennies, BattleOfTheSexes, JordanGame\n",
    "from bnelearn.environment import MatrixGameEnvironment\n",
    "from bnelearn.bidder import MatrixGamePlayer\n",
    "from bnelearn.strategy import MatrixGameStrategy,FictitiousPlayStrategy, FictitiousPlaySmoothStrategy, FictitiousPlayMixedStrategy\n",
    "from bnelearn.optimizer import ES\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = [\"FPM\",\"JG\"]\n",
    "param_tau = [0.0001,10,0.9]#[0.5,10,0.99] #[0.0001,10,0.9]\n",
    "initial_beliefs = None #torch.Tensor([[60,40],[40,60]]).to(device)\n",
    "\n",
    "options = {\"FP\": FictitiousPlayStrategy,\n",
    "           \"FPS\": FictitiousPlaySmoothStrategy,\n",
    "           \"FPM\": FictitiousPlayMixedStrategy,\n",
    "           \"PD\": PrisonersDilemma,\n",
    "           \"MP\": MatchingPennies,\n",
    "           \"BoS\": BattleOfTheSexes,\n",
    "           \"JG\": JordanGame}        \n",
    "\n",
    "run_name = time.strftime('{}_%Y-%m-%d %a %H:%M:%S'.format(setting[0]))\n",
    "game_name = setting[1]\n",
    "logdir = os.path.join(root_path, 'notebooks', 'matrix', game_name, run_name)\n",
    "logdir\n",
    "\n",
    "## Experiment setup\n",
    "n_players = 3\n",
    "epoch = 10000\n",
    "\n",
    "## Environment settings\n",
    "#Dummies here\n",
    "batch_size = 1\n",
    "input_length = 1\n",
    "\n",
    "# optimization params\n",
    "'''\n",
    "All with 10,000 epochs\n",
    "PD:\n",
    "FP [] - Converges to 0 quickly\n",
    "FPS [0.5, 10, 0.99] - Converges to [0.11,0.89]\n",
    "FPS [0.0 ,10, 0.90] - Converges to 0 quickly\n",
    "FPM [0.5, 10, 0.99] - Converges to [0.11,0.89] \n",
    "FPM [0.0, 10, 0.90] - Converges to 0 quickly \n",
    "\n",
    "MP:\n",
    "FP [] - Cycles. Historical play cycles around 0.5\n",
    "FPS [0.5, 10, 0.99] - Play converges to 0.5\n",
    "FPS [0.0 ,10, 0.90] - Cycles. Historical play cycles around 0.5\n",
    "FPM [0.5, 10, 0.99] - Play converges to 0.5\n",
    "FPM [0.0, 10, 0.90] - Cycles. Historical play cycles around 0.5\n",
    "\n",
    "BoS:\n",
    "FP [] - Converges to PNE 0 or 1\n",
    "FPS [0.0 ,10, 0.90] - Converges to PNE 0 or 1\n",
    "FPS [0.5, 10, 0.99] - Converges slowly to PNE 0 or 1\n",
    "FPM [0.0 ,10, 0.90] - Converges quickly to PNE 0 or 1\n",
    "FPM [0.5, 10, 0.99] - Converges to PNE 0 or 1\n",
    "FPS [0.0 ,10, 0.90], torch.Tensor([[60,40],[40,60]]).to(device)) - Converges to PNE 0 or 1\n",
    "FPS [0.5, 10, 0.99], torch.Tensor([[60,40],[40,60]]).to(device)) - Converges very slowly to PNE 0 or 1\n",
    "FPM [0.0 ,10, 0.90], torch.Tensor([[60,40],[40,60]]).to(device)) - Interesting. The play cycles a lot at first and plays MNE later. Hist. play is permanent in MNE.\n",
    "FPM [0.5, 10, 0.99], torch.Tensor([[60,40],[40,60]]).to(device)) - Converges very fast to [0.57, 0.43]\n",
    "\n",
    "Jordan Game:\n",
    "FP [] - Very long cycles\n",
    "FPS [0.0 ,10, 0.90] - Very long cycles\n",
    "FPS [0.5, 10, 0.99] - Quickly vonverges to 0.5\n",
    "FPM [0.0 ,10, 0.90] - Very long cycles\n",
    "FPM [0.5, 10, 0.99] - Quickly vonverges to 0.5\n",
    "'''\n",
    "tau_minimum = param_tau[0]\n",
    "tau_update_interval =  param_tau[1]\n",
    "tau_update =  param_tau[2]\n",
    "\n",
    "param = \"tau_minimum: {} \\ntau_update_interval: {} \\ntau_update: {}\".format(tau_minimum,tau_update_interval,tau_update)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "specific_gpu = 5\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "# Wrapper transforming a strategy to bidder, used by the optimizer\n",
    "# this is a dummy, valuation doesn't matter\n",
    "def strat_to_player(strategy, batch_size, player_position=None):\n",
    "    return MatrixGamePlayer(strategy, batch_size = batch_size, player_position=player_position)\n",
    "\n",
    "\n",
    "game = options[setting[1]]()\n",
    "if options[setting[0]] is FictitiousPlayStrategy or options[setting[0]] is FictitiousPlaySmoothStrategy:\n",
    "    strat1 = options[setting[0]](game = game, initial_beliefs = initial_beliefs)\n",
    "    strat2 = options[setting[0]](game = game, initial_beliefs = initial_beliefs)\n",
    "    strat3 = options[setting[0]](game = game, initial_beliefs = initial_beliefs)\n",
    "    strat = [strat1,strat2,strat3]\n",
    "else:\n",
    "    strat0 = options[setting[0]](game = game, initial_beliefs = initial_beliefs)\n",
    "    strat = [strat0,strat0,strat0]\n",
    "\n",
    "player1 = strat_to_player(strat[0], batch_size = batch_size, player_position = 0)\n",
    "player2 = strat_to_player(strat[1], batch_size = batch_size, player_position = 1)\n",
    "player3 = strat_to_player(strat[2], batch_size = batch_size, player_position = 2)\n",
    "player = [player1,player2,player3]\n",
    "\n",
    "env = MatrixGameEnvironment(game = game,\n",
    "                           agents = [player1,player2,player3],\n",
    "                           n_players = 3,\n",
    "                           batch_size = batch_size,\n",
    "                           strategy_to_player_closure = strat_to_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel updating\n",
    "print(param)\n",
    "with SummaryWriter(log_dir=logdir, flush_secs=30) as writer:\n",
    "    writer.add_text('hyperparams/hyperparameter', param, 0)  \n",
    "    torch.cuda.empty_cache()\n",
    "    for e in range(epoch):\n",
    "        actions = [None,None,None]\n",
    "        for i,playr in enumerate(player):\n",
    "            actions[i] = playr.get_action()\n",
    "        \n",
    "        if e%1000 == 0:\n",
    "            print(actions)\n",
    "\n",
    "        for _,strategy in enumerate(strat):\n",
    "            strategy.update_observations(actions)\n",
    "            strategy.update_beliefs()\n",
    "            if (type(strategy) is FictitiousPlaySmoothStrategy or type(strategy) is FictitiousPlayMixedStrategy) and e > 0 and e%tau_update_interval == 0 and strategy.tau >= tau_minimum:\n",
    "                strategy.update_tau(tau_update)\n",
    "            \n",
    "        # Logging\n",
    "        for i,playr in enumerate(player):\n",
    "            writer.add_histogram('eval/p{}_action_distribution'.format(i), actions[i].view(-1).cpu().numpy(), e)\n",
    "            # Careful: With FPM this always shows only probs_self of player 1. TODO: Change when implementing a logger\n",
    "            writer.add_scalar('eval_player_{}/prob_action_0'.format(i), playr.strategy.probs_self[0], e)\n",
    "            writer.add_scalar('eval_player_{}/hist_prob_action_0'.format(i), playr.strategy.probs[i][0], e)\n",
    "            if type(strategy) is FictitiousPlayMixedStrategy:\n",
    "                # Careful: This is currently not working. It takes the exp. util of the correct player but the probs_self is always of the last player TODO: Fix!\n",
    "                writer.add_scalar('eval_player_{}/utility'.format(i), (playr.strategy.exp_util * playr.strategy.probs_self[0]).sum(), e)\n",
    "            else:\n",
    "                writer.add_scalar('eval_player_{}/utility'.format(i), playr.strategy.exp_util[actions[i]], e)\n",
    "        \n",
    "'''\n",
    "# Sequential updating\n",
    "for e in range(epoch):\n",
    "    for i,playr in enumerate(player):\n",
    "        actions = [None,None]\n",
    "        actions[i] = playr.get_action()\n",
    "        print(actions)\n",
    "        for _,strategy in enumerate(strat):\n",
    "            strategy.update(actions)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
