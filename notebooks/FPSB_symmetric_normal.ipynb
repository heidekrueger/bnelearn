{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n Player FPSB Auction with symmetric valuation distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as ut\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from bnelearn.strategy import NeuralNetStrategy, ClosureStragegy\n",
    "from bnelearn.bidder import Bidder\n",
    "from bnelearn.mechanism import FirstPriceSealedBidAuction, VickreyAuction\n",
    "from bnelearn.optimizer import ES\n",
    "from bnelearn.environment import AuctionEnvironment\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "# Use specific cuda gpu if desired \n",
    "#(i.e. for running multiple experiments in parallel)\n",
    "specific_gpu = 5\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "print(device)\n",
    "if cuda: print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The following cell fully defines an experiment.\n",
    "\n",
    "The folowing settings are known to work well:\n",
    "```\n",
    "3p:  batch_size = 2**17, learning_rate = 5e-1, lr_decay_every = 500, lr_decay_factor = 0.7,\n",
    "     momentum = 0.5, sigma = .02, n_perturbations = 128,\n",
    "     eval_batch_size 2**15\n",
    "     hidden_nodes = [5,5], hidden_activations = [nn.SELU(), nn.SELU()]\n",
    "     \n",
    "     \n",
    "5p:  batch_size = 2**17, size_hidden_layer = 10, learning_rate = 5e-1, lr_decay_every = 1000, lr_decay_factor = 0.75,\n",
    "     momentum = 0.7, sigma = .02, n_perturbations = 256,\n",
    "     eval_batch_size 2**15\n",
    "     (1 hidden layer, tanh)\n",
    "     \n",
    "10p: batch_size = 2**17, size_hidden_layer = 10, learning_rate = 5e-1, lr_decay_every = 1000, lr_decay_factor = 0.8,\n",
    "     momentum = 0.6, sigma = .02, n_perturbations = 128,\n",
    "     eval_batch_size 2**15\n",
    "     (1 hidden layer, tanh)\n",
    "     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_comment = '' # used in log title\n",
    "\n",
    "## Experiment setup\n",
    "n_players = 3\n",
    "n_items = 1\n",
    "\n",
    "# valuation distribution\n",
    "valuation_mean = 10.0\n",
    "valuation_std = 5.0\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 2**17\n",
    "input_length = 1\n",
    "\n",
    "eval_batch_size = 2**15 # FAILS for 2**16... why?\n",
    "n_processes_optimal_strategy = 32\n",
    "\n",
    "# strategy model architecture\n",
    "hidden_nodes = [5, 5]\n",
    "hidden_activations = [nn.SELU(), nn.SELU()]\n",
    "\n",
    "# optimization params\n",
    "epoch = 5000\n",
    "learning_rate = 5e-1\n",
    "lr_decay = True\n",
    "lr_decay_every = 500\n",
    "lr_decay_factor = 0.70\n",
    "baseline = True\n",
    "momentum = 0.6\n",
    "\n",
    "sigma = .02 #ES noise parameter\n",
    "n_perturbations = 128\n",
    "\n",
    "# plot and log training options\n",
    "plot_epoch = 200 #plot and log optima this often\n",
    "write_graph = True # whether to log graph to disk\n",
    "plot_points = min(100, batch_size)\n",
    "plot_xmin = int(max(0, valuation_mean - 3*valuation_std))\n",
    "plot_xmax = int(valuation_mean + 3*valuation_std)\n",
    "\n",
    "plot_ymin = 0\n",
    "plot_ymax = 20\n",
    "\n",
    "def strat_to_bidder(strategy, batch_size=batch_size, player_position=None, cache_actions=False):\n",
    "    return Bidder.normal(valuation_mean,valuation_std, strategy,\n",
    "                         batch_size = batch_size,\n",
    "                         player_position=player_position,\n",
    "                         cache_actions=cache_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism = FirstPriceSealedBidAuction(cuda = True)\n",
    "\n",
    "model = NeuralNetStrategy(input_length,\n",
    "                          hidden_nodes = hidden_nodes,\n",
    "                          hidden_activations = hidden_activations,\n",
    "                          requires_grad=False,\n",
    "                          ensure_positive_output = torch.tensor([float(valuation_mean)])\n",
    "                          ).to(device) \n",
    "\n",
    "bidders = [strat_to_bidder(model, batch_size, player_position)\n",
    "           for player_position in range(n_players)]\n",
    "\n",
    "env = AuctionEnvironment(mechanism,\n",
    "                  agents = bidders,\n",
    "                  batch_size = batch_size,\n",
    "                  n_players =n_players,\n",
    "                  strategy_to_player_closure = strat_to_bidder\n",
    "                 )\n",
    "optimizer = ES(model=model, environment = env,\n",
    "               lr = learning_rate, momentum=momentum,\n",
    "               sigma=sigma, n_perturbations=n_perturbations)\n",
    "\n",
    "print(model)\n",
    "n_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print('Total parameters: ' + str(n_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Evaluation and logging\n",
    "\n",
    "###  Optimal Bid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Menezes et al. 2005., the optimal bid for symmetric valuations $v$ that are distributed with cdf $F(v)$ for $n$ players in this setting is given by\n",
    "\n",
    "$$b^*(v) = v - \\frac{\\int_0^v F(x)^{n-1} dx}{F(v)^{n-1}} $$\n",
    "\n",
    "We implement it here for calculating comparison metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "import warnings\n",
    "\n",
    "common_dist = torch.distributions.normal.Normal(loc = valuation_mean, scale = valuation_std)\n",
    "\n",
    "# TODO: investigate where everything is allocated. possibly move GPU vectors to CPU completely instead of shuffling around for integration?\n",
    "def optimal_bid(valuation: torch.Tensor or np.ndarray or float) -> torch.Tensor:\n",
    "    \n",
    "    # For float and numpy --> convert to tensor\n",
    "    if not isinstance(valuation, torch.Tensor):\n",
    "        valuation = torch.tensor(valuation, dtype = torch.float)           \n",
    "    # For float / 0d tensors --> unsqueeze to allow list comprehension below\n",
    "    if valuation.dim() == 0:\n",
    "        valuation.unsqueeze_(0)\n",
    "    \n",
    "    # shorthand notation for F^(n-1)\n",
    "    Fpowered = lambda v: torch.pow(common_dist.cdf(v), n_players - 1)  \n",
    "    \n",
    "    # do the calculations\n",
    "    numerator = torch.tensor(\n",
    "            [integrate.quad(Fpowered, 0, v)[0] for v in valuation],\n",
    "            device = valuation.device\n",
    "        ).reshape(valuation.shape)                                 \n",
    "    return valuation - numerator / Fpowered(valuation)\n",
    "\n",
    "# equilibrium Strategy and environment in equilibirum\n",
    "bneStrategy = ClosureStragegy(optimal_bid, parallel=n_processes_optimal_strategy)\n",
    "# set up an environment populated by players that play the optimum\n",
    "bne_env = AuctionEnvironment(mechanism,\n",
    "                             agents = [strat_to_bidder(bneStrategy,\n",
    "                                                       player_position= i,\n",
    "                                                       batch_size = eval_batch_size,\n",
    "                                                       cache_actions = True) \n",
    "                                       for i in range(n_players)\n",
    "                                      ],\n",
    "                             batch_size = eval_batch_size,\n",
    "                             n_players=n_players,\n",
    "                             strategy_to_player_closure = strat_to_bidder\n",
    "                             )\n",
    "\n",
    "with warnings.catch_warnings(): \n",
    "    warnings.simplefilter('ignore')\n",
    "    # don't print scipy accuracy warnings\n",
    "    bne_utility = bne_env.get_reward(bne_env.agents[0], draw_valuations = True)\n",
    "print('Utility in BNE: \\t\\t{:.5f}'.format(bne_utility))\n",
    "utility_vs_bne = bne_env.get_strategy_reward(model, player_position=0)\n",
    "print('Model utility vs BNE: \\t\\t{:.5f}'.format(utility_vs_bne))\n",
    "utility_learning_env = env.get_strategy_reward(model, player_position=0, draw_valuations = True)\n",
    "print('Model utility in learning env:\\t{:.5f}'.format(utility_learning_env))\n",
    "  \n",
    "    \n",
    "# predefine points for plotting optimal curve to save cpu-bound integrations\n",
    "v_opt = np.linspace(plot_xmin, plot_xmax, 100) # 100 points more than enough\n",
    "b_opt = optimal_bid(v_opt).numpy()\n",
    "\n",
    "def plot_bid_function(fig, v,b, writer=None, e=None, plot_points=plot_points):\n",
    "    \n",
    "    # subsample points and plot\n",
    "    v = v.detach().cpu().numpy()[:plot_points]\n",
    "    b= b.detach().cpu().numpy()[:plot_points]\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    plt.cla()\n",
    "    plt.xlim(plot_xmin, plot_xmax)\n",
    "    plt.ylim(plot_ymin, plot_ymax)\n",
    "    plt.plot(v,b, 'o', v_opt, b_opt, 'r--')\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    if writer:\n",
    "        writer.add_figure('eval/bid_function', fig, e)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot optimal bid to ensure appropriate boundaries have been chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bid_function(None, torch.tensor([0]),torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_once(writer, e):\n",
    "    \"\"\"Everything that should be logged only once on initialization.\"\"\"\n",
    "    writer.add_scalar('debug/total_model_parameters', n_parameters, e)\n",
    "    writer.add_text('hyperparams/neural_net_spec', str(model), 0)    \n",
    "    writer.add_scalar('debug/eval_batch_size', eval_batch_size, e)\n",
    "    writer.add_graph(model, env.agents[0].valuations)   \n",
    "\n",
    "def log_hyperparams(writer, e):\n",
    "    writer.add_scalar('hyperparams/batch_size', batch_size, e)\n",
    "    writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "    writer.add_scalar('hyperparams/momentum', momentum, e)\n",
    "    writer.add_scalar('hyperparams/sigma', sigma, e)\n",
    "    writer.add_scalar('hyperparams/n_perturbations', n_perturbations, e)\n",
    "\n",
    "def training_loop(e, writer):\n",
    "    \n",
    "    global overhead_mins, learning_rate\n",
    "    \n",
    "    if lr_decay and e % lr_decay_every == 0 and e > 0:\n",
    "        learning_rate = learning_rate * lr_decay_factor\n",
    "        log_hyperparams(writer, e)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    \n",
    "    ### do in every iteration ###\n",
    "    # save current params to calculate update norm\n",
    "    prev_params = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    #update model\n",
    "    utility = -optimizer.step()\n",
    "    # calculate infinity-norm of update step\n",
    "    new_params = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    update_norm = (new_params - prev_params).norm(float('inf'))    \n",
    "\n",
    "    # calculate utility vs bne    \n",
    "    utility_vs_bne = bne_env.get_reward(strat_to_bidder(model, batch_size = eval_batch_size), draw_valuations=False)\n",
    "    epsilon_relative = 1 - utility_vs_bne / bne_utility\n",
    "    epsilon_absolute = bne_utility - utility_vs_bne\n",
    "    \n",
    "    #log those things\n",
    "    writer.add_scalar('eval/utility', utility, e)\n",
    "    writer.add_scalar('debug/norm_parameter_update', update_norm, e)\n",
    "    writer.add_scalar('eval/utility_vs_bne', utility_vs_bne, e)\n",
    "    writer.add_scalar('eval/epsilon_relative', epsilon_relative, e)\n",
    "    writer.add_scalar('debug/epsilon_absolute', epsilon_absolute, e) # debug because only interesting to see if numeric precision is a problem, otherwise same as relative but scaled.\n",
    "\n",
    "    if e % plot_epoch == 0:\n",
    "        start_time = timer()\n",
    "        # plot current function output\n",
    "        v = bidders[0].valuations\n",
    "        b = bidders[0].get_action()\n",
    "\n",
    "        #writer.add_graph(model, bidder.valuations) \n",
    "\n",
    "        print(\"Epoch {}: \\tcurrent utility: {:.3f},\\t utility vs BNE: {:.3f}, \\tepsilon (abs/rel): ({:.5f}, {:.5f})\".format(e, utility, utility_vs_bne, epsilon_absolute, epsilon_relative))\n",
    "        plot_bid_function(fig, v,b,writer,e)\n",
    "            \n",
    "        elapsed = timer() - start_time\n",
    "            \n",
    "        overhead_mins = overhead_mins + elapsed/60\n",
    "        writer.add_scalar('debug/overhead_mins', overhead_mins, e)\n",
    "            \n",
    "        print(\"Logging checkpoint took {:.2f}s.\".format(elapsed))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not 'resume' in locals() or not resume:\n",
    "#    e = 0\n",
    "#    overhead_mins = 0\n",
    "\n",
    "# setup logger\n",
    "if os.name == 'nt': raise ValueError('The run_name may not contain : on Windows! (change datetime format to fix this)') \n",
    "run_name = time.strftime('%Y-%m-%d %a %H:%M')\n",
    "if run_comment:\n",
    "    run_name = run_name + ' - ' + str(run_comment)\n",
    "logdir = os.path.join(root_path, 'notebooks', 'fpsb', str(n_players) + 'p', 'normal', 'symmetric', run_name)\n",
    "\n",
    "e = 0\n",
    "overhead_mins = 0\n",
    "\n",
    "print(logdir)\n",
    "fig = plt.figure()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with SummaryWriter(logdir, flush_secs=30) as writer:\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    log_once(writer, 0)\n",
    "    log_hyperparams(writer, 0)    \n",
    "    \n",
    "    for e in range(e,e+epoch+1):\n",
    "        training_loop(e, writer)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
