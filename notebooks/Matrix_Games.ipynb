{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Simple Games with Neural Nets\n",
    "\n",
    "In this notebook, we implement equilibria learning viea self play for simple games such as Battle of the Sexes and Matching Pennies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    \n",
    "import torch\n",
    "from bnelearn.strategy import MatrixGameStrategy\n",
    "from bnelearn.bidder import Bidder, Player, MatrixGamePlayer\n",
    "from bnelearn.mechanism import PrisonersDilemma, BattleOfTheSexes, MatchingPennies, RockPaperScissors\n",
    "from bnelearn.optimizer import ES\n",
    "from bnelearn.environment import Environment, AuctionEnvironment, MatrixGameEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Game: Prisoners' Dilemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = time.strftime('%Y-%m-%d %a %H:%M')\n",
    "logdir = os.path.join(root_path, 'notebooks', 'matrix', 'pd', run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment setup\n",
    "n_players = 2\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 64\n",
    "input_length = 1\n",
    "\n",
    "\n",
    "# optimization params\n",
    "epoch = 25\n",
    "learning_rate = 1\n",
    "lr_decay = False\n",
    "lr_decay_every = 1000\n",
    "lr_decay_factor = 0.8\n",
    "\n",
    "sigma = 5 #ES noise parameter\n",
    "n_perturbations = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper transforming a strategy to bidder, used by the optimizer\n",
    "# this is a dummy, valuation doesn't matter\n",
    "def strat_to_player(strategy, batch_size, player_position=None):\n",
    "    return MatrixGamePlayer(strategy, batch_size = batch_size, player_position=player_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatrixGameStrategy(n_actions=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = PrisonersDilemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MatrixGameEnvironment(game, \n",
    "                 agents=[strat_to_player(model, batch_size, i) for i in range(n_players)],\n",
    "                 n_players=2,\n",
    "                 batch_size=batch_size,\n",
    "                 strategy_to_player_closure=strat_to_player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ES(model=model, environment = env, lr = learning_rate, sigma=sigma, n_perturbations=n_perturbations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyperparams(writer):\n",
    "    writer.add_scalar('hyperparams/batch_size', batch_size)\n",
    "    writer.add_scalar('hyperparams/learning_rate', learning_rate)\n",
    "    writer.add_scalar('hyperparams/sigma', sigma)\n",
    "    writer.add_scalar('hyperparams/n_perturbations', n_perturbations)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(log_dir=logdir, flush_secs=30) as writer:\n",
    "    torch.cuda.empty_cache()\n",
    "    log_hyperparams(writer)\n",
    "\n",
    "    for e in range(epoch+1):    \n",
    "\n",
    "        # lr decay?\n",
    "        if lr_decay and e % lr_decay_every == 0 and e > 0:\n",
    "            learning_rate = learning_rate * lr_decay_factor\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "            writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "\n",
    "        # always: do optimizer step\n",
    "        utility = -optimizer.step()\n",
    "        writer.add_scalar('eval/utility', utility, e) \n",
    "        writer.add_scalar('eval/prob_action_0', model.distribution.probs[0], e)    \n",
    "        #print(list(model.named_parameters()))\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = strat_to_player(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.get_action().float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assymmetric Games, BoS and Matching Pennies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment setup\n",
    "n_players = 2\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 2**10\n",
    "input_length = 1\n",
    "\n",
    "\n",
    "# optimization params\n",
    "epoch = 1000\n",
    "learning_rate = 1\n",
    "lr_decay = False\n",
    "lr_decay_every = 100\n",
    "lr_decay_factor = 0.8\n",
    "\n",
    "sigma = 5 #ES noise parameter\n",
    "n_perturbations = 10\n",
    "\n",
    "game = MatchingPennies()\n",
    "directory_name = 'matching_pennies'\n",
    "n_actions = 2\n",
    "\n",
    "game= BattleOfTheSexes()\n",
    "game_name = 'bos'\n",
    "n_actions =2\n",
    "\n",
    "game = RockPaperScissors()\n",
    "game_name = 'rps'\n",
    "n_actions = 3\n",
    "\n",
    "run_name = time.strftime('%Y-%m-%d %a %H:%M')\n",
    "logdir = os.path.join(root_path, 'notebooks', 'matrix', game_name, run_name)\n",
    "print(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper transforming a strategy to bidder, used by the optimizer\n",
    "# this is a dummy, valuation doesn't matter\n",
    "def strat_to_player(strategy, batch_size, player_position=None):\n",
    "    return MatrixGamePlayer(strategy, batch_size = batch_size,  player_position=player_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MatrixGameStrategy(n_actions=n_actions).cuda()\n",
    "model2 = MatrixGameStrategy(n_actions=n_actions).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MatrixGameEnvironment(game, agents=[model1, model2],\n",
    "                 n_players=2,\n",
    "                 batch_size=batch_size,\n",
    "                 strategy_to_player_closure=strat_to_player\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = ES(model=model1, environment = env, lr = learning_rate, sigma=sigma, n_perturbations=n_perturbations, strat_to_player_kwargs={'player_position':0})\n",
    "optimizer2 = ES(model=model2, environment = env, lr = learning_rate, sigma=sigma, n_perturbations=n_perturbations, strat_to_player_kwargs={'player_position':1})\n",
    "optimizers = [optimizer1, optimizer2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hyperparams(writer):\n",
    "    writer.add_scalar('hyperparams/batch_size', batch_size)\n",
    "    writer.add_scalar('hyperparams/learning_rate', learning_rate)\n",
    "    writer.add_scalar('hyperparams/sigma', sigma)\n",
    "    writer.add_scalar('hyperparams/n_perturbations', n_perturbations)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.distribution.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.distribution.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_utility_1 = 0\n",
    "hist_utility_2 = 0\n",
    "with SummaryWriter(log_dir=logdir) as writer:\n",
    "    torch.cuda.empty_cache()\n",
    "    log_hyperparams(writer)\n",
    "\n",
    "    for e in range(epoch+1):    \n",
    "\n",
    "        # lr decay?\n",
    "        if lr_decay and e % lr_decay_every == 0 and e > 0:\n",
    "            learning_rate = learning_rate * lr_decay_factor\n",
    "            writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "            for optimizer in optimizers:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = learning_rate\n",
    "\n",
    "        # always: do optimizer step\n",
    "        utility1 = -optimizer1.step()     \n",
    "        utility2 = -optimizer2.step()\n",
    "        \n",
    "        \n",
    "        if e > 0:\n",
    "            hist_utility_1 = (e * hist_utility_1 + utility1)/ (e+1)\n",
    "            hist_utility_2 = (e * hist_utility_2 + utility2)/ (e+1)\n",
    "        else:\n",
    "            hist_utility_1 = utility1\n",
    "            hist_utility_2 = utility2\n",
    "            \n",
    "        writer.add_histogram('eval/p1_action_distribution', env.agents[0].get_action().view(-1).cpu().numpy(), e)\n",
    "            \n",
    "        writer.add_scalar('eval_player_1/utility', utility1, e)\n",
    "        writer.add_scalar('eval_player_1/historic_utility', hist_utility_1, e) \n",
    "        writer.add_scalar('eval_player_1/prob_action_0', model1.distribution.probs[0], e)\n",
    "        \n",
    "        writer.add_scalar('eval_player_2/utility', utility2, e)\n",
    "        writer.add_scalar('eval_player_2/historic_utility', hist_utility_2, e)\n",
    "        writer.add_scalar('eval_player_2/prob_action_0', model2.distribution.probs[0], e)\n",
    "        #print(list(model.named_parameters()))\n",
    "        if not e % 50: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(utility1.item(), utility2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hist_utility_1.item(), hist_utility_2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
