{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of different ES implementations\n",
    "\n",
    "e.g. how to standardize, which sigma to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\sigma$ in the update step\n",
    "\n",
    "Is the division by $\\sigma$ in Salimans et al correct? **yes**\n",
    "\n",
    "Is there a difference in std-deviation of the estimates whether we draw from $N(0,1)$ and divide by $\\sigma$ or draw from $N(0,\\sigma)$ and divide by $\\sigma^2$? **--> no**\n",
    "\n",
    "How to choose $\\sigma$ to minimize variance? \n",
    "**unclear**, _for each f and x there's some sigma that have lower variance than others. 1/n\\_params or a fixed share of mean parameter size seems like a good guess, but this likely depends on normalization of fitness, which seems to be 1 or 2 orders of magnitude more important._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([2., -2., 5.], device='cuda')\n",
    "def f(x):\n",
    "    return sum([weights[i]*x**i for i in range(len(weights))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8., device='cuda:0')"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analytical Gradient\n",
    "x = torch.tensor(1.0, requires_grad=True, device = 'cuda')\n",
    "y = f(x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1 #float(x)/20+1e-1\n",
    "npop = int(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES gradient\n",
    "def es_grad_salimans():\n",
    "    eps = torch.zeros(npop).normal_(0, 1)\n",
    "    fitness = f(x+sigma*eps).detach()\n",
    "    return (fitness*eps).mean()/sigma\n",
    "\n",
    "def es_grad_ours():\n",
    "    eps = torch.zeros(npop).normal_(0, sigma)\n",
    "    fitness = f(x+eps).detach()\n",
    "    return (fitness*eps).mean()/sigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "theirs = torch.cat([es_grad_salimans().unsqueeze(0) for _ in range(1000)])\n",
    "ours = torch.cat([es_grad_ours().unsqueeze(0) for _ in range(1000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(8.0112) tensor(0.2617) tensor(0.0327)\n",
      "tensor(7.9964) tensor(0.2617) tensor(0.0327)\n"
     ]
    }
   ],
   "source": [
    "print(sigma)\n",
    "print(theirs.mean(), theirs.std(), theirs.std()/theirs.mean())\n",
    "print(ours.mean(), ours.std(), ours.std()/ours.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type or normalization\n",
    "What's better, reward-scaling or using current utility as baseline?\n",
    "Does sigma matter?\n",
    "\n",
    "* **Scaling no longer provides a gradient estimate but just a direction**. For small enough sigma and dividing by sigma **once**, this direction has length ~1.\n",
    "* Current fitness as baseline vs shifting by the mean has no effect on variance for reasonably small choices of sigma. For very large sigma, the mean provides less variance than the baseline utility. (See caveat below!)\n",
    "*  However, the mean also seems to give _biased_ estimate for small npop. (Try 10k iterations of x=-0.5, sigma=0.1, npop=100 (or even npop=10). Then gradf=-7, but shifted consistently returns ~-6.9 over 10k iterations of n_pop. It turns out, that's becuase in general $$\\mathbb E[\\varepsilon F(x+\\varepsilon)] \\neq 0 $$, so the mean is **not** a valid baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true gradient: \t tensor(-27.)\n",
      "no baseline:\t tensor(-27.2898) tensor(384.1292)\n",
      "fixed baseline:\t tensor(-26.9688) tensor(3.8177)\n",
      "mean baseline:\t tensor(-26.7394) tensor(3.7991) with n-1 in divisor: tensor(-27.0095)\n",
      "scaled rewards:\t tensor(-0.9878) tensor(0.0704)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(-2.5, requires_grad=True, device = 'cuda')\n",
    "sigma = 0.01\n",
    "npop = int(100)\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "def es_grad():\n",
    "    eps = torch.zeros(npop).normal_(0, sigma)\n",
    "    fitness = f(x+eps).detach()\n",
    "    return (fitness*eps).mean()/sigma**2\n",
    "\n",
    "def es_grad_baseline():\n",
    "    eps = torch.zeros(npop).normal_(0, sigma)\n",
    "    fitness = f(x+eps).detach()\n",
    "    return ((fitness-y.detach())*eps).mean()/sigma**2\n",
    "\n",
    "def es_grad_shifted():\n",
    "    eps = torch.zeros(npop).normal_(0, sigma)\n",
    "    fitness = f(x+eps).detach()\n",
    "    mean = fitness.mean()\n",
    "    return ((fitness-mean)*eps).mean()/sigma**2\n",
    "\n",
    "def es_grad_scaled():\n",
    "    eps = torch.zeros(npop).normal_(0, sigma)\n",
    "    fitness = f(x+eps).detach()\n",
    "    return ((fitness-fitness.mean())/fitness.std()*eps).mean()/sigma\n",
    "\n",
    "no_base  = torch.cat([es_grad().unsqueeze(0) for _ in range(100000)])\n",
    "baseline = torch.cat([es_grad_baseline().unsqueeze(0) for _ in range(100000)])\n",
    "shifted  = torch.cat([es_grad_shifted().unsqueeze(0) for _ in range(100000)])\n",
    "scaled   = torch.cat([es_grad_scaled().unsqueeze(0) for _ in range(100000)])\n",
    "\n",
    "print(str('true gradient: \\t'), x.grad.cpu())\n",
    "print(str('no baseline:\\t'), no_base.mean(), no_base.std())\n",
    "print(str('fixed baseline:\\t'), baseline.mean(), baseline.std())\n",
    "print(str('mean baseline:\\t'), shifted.mean(), shifted.std(), 'with n-1 in divisor:', shifted.mean() * npop/(npop-1))\n",
    "print(str('scaled rewards:\\t'), scaled.mean(), scaled.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_wat():\n",
    "    eps = torch.zeros(npop).normal_(0, sigma)\n",
    "    fitness = f(x+eps).detach()\n",
    "    mean = fitness.mean()\n",
    "    return mean*eps/sigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npop = int(100)\n",
    "sigma = 10\n",
    "wat= torch.cat([es_wat().unsqueeze(0) for _ in range(100000)])\n",
    "wat.mean(), wat.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
