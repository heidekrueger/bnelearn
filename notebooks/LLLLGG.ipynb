{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################Imports###################################\n",
    "import os\n",
    "import sys\n",
    "root_path = os.path.join(os.path.expanduser('~'), 'bnelearn')\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as ut\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from bnelearn.strategy import NeuralNetStrategy, ClosureStrategy\n",
    "from bnelearn.bidder import Bidder\n",
    "from bnelearn.mechanism import LLLLGGAuction, CombinatorialAuction\n",
    "from bnelearn.learner import ESPGLearner\n",
    "from bnelearn.environment import AuctionEnvironment\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "    \n",
    "cuda = False #torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "# Use specific cuda gpu if desired (i.e. for running multiple experiments in parallel)\n",
    "specific_gpu = 2\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "print(device)\n",
    "if cuda: print(torch.cuda.current_device())\n",
    "\n",
    "##################################Settings##################################\n",
    "# log in notebook folder\n",
    "# alternative for shared access to experiments:\n",
    "#log_root = os.path.abspath('/srv/bnelearn/experiments')\n",
    "log_root = os.path.abspath('.')\n",
    "run_comment = 'espg'\n",
    "save_figure_data_to_disk = False\n",
    "save_figure_to_disk = False\n",
    "\n",
    "## Experiment setup\n",
    "n_players = 6\n",
    "n_items = 8\n",
    "# valuation distribution\n",
    "# both players should have same lower bound\n",
    "u_lo =   0.0\n",
    "u0_hi = 1.\n",
    "u1_hi = 2.\n",
    "u_his = [u0_hi, u0_hi, u0_hi, u0_hi, u1_hi, u1_hi]\n",
    "\n",
    "def strat_to_bidder(strategy, batch_size, player_position):\n",
    "    return Bidder.uniform(u_lo, u_his[player_position], strategy, n_items = 2, player_position=player_position, batch_size = batch_size, cuda = cuda)\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 2**12\n",
    "eval_batch_size = 2**25\n",
    "epoch = 2\n",
    "\n",
    "# strategy model architecture\n",
    "n_threads = 1\n",
    "core_solver = 'cvxpy'\n",
    "pricing_rule = 'nearest-vcg'\n",
    "input_length = 2\n",
    "hidden_nodes = [2]#5, 5]\n",
    "hidden_activations = [nn.SELU()]#, nn.SELU()]#, nn.SELU()]\n",
    "\n",
    "\n",
    "learner_hyperparams = {\n",
    "    'population_size':4,\n",
    "    'sigma': 1.,\n",
    "    'scale_sigma_by_model_size': True\n",
    "}\n",
    "\n",
    "### Optimizer hyperparams\n",
    "# SGD standards\n",
    "    #'lr': 1e-3,\n",
    "    #'momentum': 0.7\n",
    "# Adam standards:\n",
    "    # 'lr': 1e-3\n",
    "    # 'betas': (0.9, 0.999), #coefficients for running avgs of grad and square grad\n",
    "    # 'eps': 1e-8 , # added to denominator for numeric stability\n",
    "    # 'weight_decay': 0, #L2-decay\n",
    "    # 'amsgrad': False #whether to use amsgrad-variant\n",
    "optimizer_type = torch.optim.SGD\n",
    "optimizer_hyperparams ={    \n",
    "    'lr': 1e-3,\n",
    "    'momentum': 0.9\n",
    "}\n",
    "\n",
    "# plot and log training options\n",
    "plot_epoch = 10\n",
    "plot_points = min(100, batch_size)\n",
    "\n",
    "plot_xmin = u_lo\n",
    "plot_xmax = u1_hi\n",
    "plot_ymin = 0\n",
    "plot_ymax = 4\n",
    "\n",
    "############################Setting up the Environment##########################\n",
    "# for evaluation\n",
    "# helper constant\n",
    "c = 1 / (u0_hi - u_lo)**2 - 1 / (u1_hi - u_lo)**2\n",
    "\n",
    "def log_once(writer, e):\n",
    "    \"\"\"Everything that should be logged only once on initialization.\"\"\"\n",
    "    for i in range(n_players):\n",
    "        #writer.add_scalar('debug_players/p{}_model_parameters'.format(i), n_parameters[i], e)\n",
    "        #writer.add_scalar('debug/model_parameters', sum(n_parameters), e)\n",
    "        writer.add_scalar('debug/eval_batch_size', eval_batch_size, e)\n",
    "        writer.add_text('hyperparams/neural_net_spec', str(model_1), 0)    \n",
    "        #writer.add_scalar('debug/eval_batch_size', eval_batch_size, e)\n",
    "        #writer.add_graph(model_1, env.agents[0].valuations)    \n",
    "\n",
    "def setup_custom_scalar_plots(writer):    \n",
    "    ## define layout first, then call add_custom_scalars once\n",
    "    layout = {'eval':\n",
    "        {\n",
    "            #'Loss vs BNE relative': ['Multiline',\n",
    "            #                         ['eval_players/p{}_epsilon_relative'.format(i) for i in range(n_players)]]\n",
    "            #'How to make a margin chart': ['Margin', ['tag_mean', 'tag_min', 'tag_max']]\n",
    "        }\n",
    "    }    \n",
    "    writer.add_custom_scalars(layout) \n",
    "\n",
    "def log_once(writer, e):\n",
    "    \"\"\"Everything that should be logged only once on initialization.\"\"\"\n",
    "    pass\n",
    "    #for i in range(n_players):\n",
    "        #writer.add_scalar('debug_players/p{}_model_parameters'.format(i), n_parameters[i], e)\n",
    "    #writer.add_scalar('debug/model_parameters', sum(n_parameters), e)\n",
    "    #writer.add_scalar('debug/eval_batch_size', eval_batch_size, e)\n",
    "    #for a in agents:\n",
    "        #writer.add_text('hyperparams/neural_net_spec', str(a.strategy), 0)    \n",
    "    #writer.add_scalar('debug/eval_batch_size', eval_batch_size, e)\n",
    "    #writer.add_graph(model_l1, env.agents[0].valuations)    \n",
    "\n",
    "def log_hyperparams(writer, e):\n",
    "#     writer.add_scalar('hyperparams/batch_size', batch_size, e)\n",
    "#     writer.add_scalar('hyperparams/batch_size', batch_size, e)\n",
    "#     writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "#     writer.add_scalar('hyperparams/momentum', momentum, e)\n",
    "#     writer.add_scalar('hyperparams/sigma', sigma, e)\n",
    "#     writer.add_scalar('hyperparams/n_perturbations', n_perturbations, e)\n",
    "    pass\n",
    "        \n",
    "def plot_bid_function(fig, valuations, bids, writer=None, e=None,\n",
    "                      plot_points=plot_points,\n",
    "                      save_vectors_to_disk=save_figure_data_to_disk,\n",
    "                      save_png_to_disk = False):\n",
    "    #$$$TODO: Removed [:plot_points] for now to get it running. Include (for batch!?) again.\n",
    "    #plot_points = min(plot_points, len(v1), len(v2))\n",
    "    # subsample points and plot    \n",
    "    v_print = [None] * 2\n",
    "    b_print = [None] * 2\n",
    "    #for k in range(len(valuations)):\n",
    "    #    v_print[k] = [None] * len(valuations[k][0])\n",
    "    #    b_print[k] = [None] * len(valuations[k][0])\n",
    "     #   for k2 in range(len(valuations[k][0])):\n",
    "    #        v_print[k][k2] = valuations[k][:,k2].detach().cpu().numpy()\n",
    "    #        b_print[k][k2] = bids[k][:,k2].detach().cpu().numpy()\n",
    "    for k in range(len(valuations)):\n",
    "        for k2 in range(len(valuations[k][0])):\n",
    "            if k==0:\n",
    "                v_print[k2] = valuations[k][:,k2].detach().cpu().numpy()\n",
    "                b_print[k2] = bids[k][:,k2].detach().cpu().numpy()\n",
    "            else:\n",
    "                v_print[k2] = np.concatenate((v_print[k2],valuations[k][:,k2].detach().cpu().numpy()))\n",
    "                b_print[k2] = np.concatenate((b_print[k2],bids[k][:,k2].detach().cpu().numpy()))\n",
    "        \n",
    "    fig = plt.gcf()\n",
    "    plt.cla()\n",
    "    plt.xlim(plot_xmin, plot_xmax)\n",
    "    plt.ylim(plot_ymin, plot_ymax)\n",
    "    plt.xlabel('valuation')\n",
    "    plt.ylabel('bid')\n",
    "    plt.text(plot_xmin + 1, plot_ymax - 1, 'iteration {}'.format(e))\n",
    "    plt.plot(v_print[0], b_print[0], 'bo')\n",
    "    #plt.plot(v_print[0][0], b_print[0][0], 'bo', v_print[1][0], b_print[1][0], 'go', v_print[2][0], b_print[2][0], 'ro', v_print[3][0], b_print[3][0], 'yo', v_print[4][0], b_print[4][0], 'g-', v_print[5][0], b_print[5][0], 'b-')\n",
    "    #plt.plot(v_print[0][1], b_print[0][1], 'bo', v_print[1][1], b_print[1][1], 'go', v_print[2][1], b_print[2][1], 'ro', v_print[3][1], b_print[3][1], 'yo', v_print[4][1], b_print[4][1], 'g-', v_print[5][1], b_print[5][1], 'b-')\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    plt.show()\n",
    "    plt.plot(v_print[1], b_print[1], 'bo')\n",
    "    display.display(fig)\n",
    "    plt.show()\n",
    "    if save_png_to_disk:\n",
    "        plt.savefig(os.path.join(logdir, 'png', run_name + f'_{e:05}.png'))\n",
    "    if writer:\n",
    "        writer.add_figure('eval/bid_function', fig, e)  \n",
    "\n",
    "##########################################################################\n",
    "# initialize models\n",
    "model_0 = NeuralNetStrategy(input_length,                            \n",
    "                            hidden_nodes = hidden_nodes,\n",
    "                            hidden_activations = hidden_activations,\n",
    "                            output_length = 2).to(device)#,\n",
    "                            #ensure_positive_output = torch.tensor([float(u_lo), float(u_lo)])).to(device)\n",
    "   \n",
    "\n",
    "model_1 = NeuralNetStrategy(input_length,\n",
    "                            hidden_nodes = hidden_nodes,\n",
    "                            hidden_activations = hidden_activations,\n",
    "                            output_length = 2).to(device)\n",
    "                            #ensure_positive_output = torch.tensor([float(u_lo), float(u_lo)])).to(device)\n",
    "\n",
    "n_parameters = [sum([p.numel() for p in model_0.parameters()]),sum([p.numel() for p in model_0.parameters()])]\n",
    "\n",
    "bidder_0 = strat_to_bidder(model_0, batch_size, player_position=0)\n",
    "bidder_1 = strat_to_bidder(model_0, batch_size, player_position=1)\n",
    "bidder_2 = strat_to_bidder(model_0, batch_size, player_position=2)\n",
    "bidder_3 = strat_to_bidder(model_0, batch_size, player_position=3)\n",
    "bidder_4 = strat_to_bidder(model_1, batch_size, player_position=4)\n",
    "bidder_5 = strat_to_bidder(model_1, batch_size, player_position=5)\n",
    "\n",
    "bidders = [bidder_0,bidder_1,bidder_2,bidder_3,bidder_4,bidder_5]\n",
    "\n",
    "mechanism = LLLLGGAuction(batch_size = batch_size, rule = pricing_rule, \n",
    "                         cuda = cuda, core_solver = core_solver, parallel = n_threads)\n",
    "env = AuctionEnvironment(mechanism,\n",
    "                  agents = bidders,\n",
    "                  batch_size = batch_size,\n",
    "                  n_players =n_players,\n",
    "                  strategy_to_player_closure = strat_to_bidder\n",
    "                 )\n",
    "learner_0 = ESPGLearner(\n",
    "    model = model_0,\n",
    "    environment = env,\n",
    "    hyperparams = learner_hyperparams,\n",
    "    optimizer_type = optimizer_type,\n",
    "    optimizer_hyperparams = optimizer_hyperparams)\n",
    "\n",
    "learner_1 = ESPGLearner(\n",
    "    model = model_1,\n",
    "    environment = env,\n",
    "    hyperparams = learner_hyperparams,\n",
    "    optimizer_type = optimizer_type,\n",
    "    optimizer_hyperparams = optimizer_hyperparams)\n",
    "\n",
    "\n",
    "\n",
    "print(model_0)\n",
    "print('Total parameters: ' + str(n_parameters))\n",
    "\n",
    "v = [None] * len(bidders)\n",
    "b = [None] * len(bidders)\n",
    "for k, bidder in enumerate(bidders):\n",
    "    bidder.draw_valuations_()\n",
    "    v[k] = bidder.valuations.squeeze(0)\n",
    "    b[k] = bidder.get_action().squeeze(0)\n",
    "\n",
    "fig = plt.figure()\n",
    "plot_bid_function(fig, v, b, writer=None,e=0) \n",
    "\n",
    "###################################################Training####################################################\n",
    "print(log_root)\n",
    "\n",
    "if os.name == 'nt': raise ValueError('The run_name may not contain : on Windows! (change datetime format to fix this)') \n",
    "run_name = time.strftime('%Y-%m-%d %a %H:%M')\n",
    "if run_comment:\n",
    "    run_name = run_name + ' - ' + str(run_comment)\n",
    "logdir = os.path.join(log_root, 'LLLLGG', 'asymmetric', 'uniform', str(n_players) + 'p', run_name)\n",
    "print(logdir)\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "if save_figure_to_disk:\n",
    "    os.mkdir(os.path.join(logdir, 'png'))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun\n",
    "\n",
    "with SummaryWriter(logdir, flush_secs=60) as writer:\n",
    "       \n",
    "    \n",
    "    overhead_mins = 0\n",
    "    torch.cuda.empty_cache()\n",
    "    log_once(writer, 0)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "\n",
    "\n",
    "    for e in range(epoch+1):\n",
    "        print(e)\n",
    "\n",
    "        # always: do optimizer step\n",
    "        utility_0 = learner_0.update_strategy_and_evaluate_utility()\n",
    "        utility_1 = learner_1.update_strategy_and_evaluate_utility()\n",
    "        \n",
    "        #logging \n",
    "        start_time = timer()\n",
    "        utilities = torch.tensor([utility_0, utility_1])\n",
    "            \n",
    "        # plot current function output\n",
    "            \n",
    "        v = [None] * len(bidders)\n",
    "        b = [None] * len(bidders)\n",
    "        for k, bidder in enumerate(bidders):\n",
    "            bidder.draw_valuations_()\n",
    "            v[k] = bidder.valuations#.squeeze(0)\n",
    "            b[k] = bidder.get_action()#.squeeze(0)\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        #print(('Epoch: {}: Model utility in learning env:'+'\\t{:.5f}'*n_players).format(e, *utilities))            \n",
    "        #print(\"Epoch {}: \\tutilities: \\t p0: {:.3f} \\t p1: {:.3f}\".format(e, utility_0, utility_1))\n",
    "        plot_bid_function(fig, v, b, writer,e,\n",
    "                              save_png_to_disk=save_figure_to_disk)  \n",
    "        \n",
    "        elapsed = timer() - start_time\n",
    "        overhead_mins = overhead_mins + elapsed/60\n",
    "        writer.add_scalar('debug/overhead_mins', overhead_mins, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
