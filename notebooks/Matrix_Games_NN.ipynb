{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os, sys, time\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "    \n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from bnelearn.strategy import MatrixGameStrategy, Strategy\n",
    "from bnelearn.bidder import MatrixGamePlayer\n",
    "from bnelearn.mechanism import PrisonersDilemma, BattleOfTheSexes, MatchingPennies, RockPaperScissors, JordanGame\n",
    "from bnelearn.learner import ESPGLearner as ES\n",
    "from bnelearn.environment import MatrixGameEnvironment\n",
    "\n",
    "from bnelearn.util.logging import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "specific_gpu = 5\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "setting = [\"None\",\"MP\"]\n",
    "initial_beliefs = [torch.tensor([[50.5],[49.5]],device='cpu'), torch.tensor([[49.5],[50.5]], device='cpu')]\n",
    "weight_normalization = True\n",
    "\n",
    "options = {\"PD\": PrisonersDilemma,\n",
    "           \"MP\": MatchingPennies,\n",
    "           \"BoS\": BattleOfTheSexes,\n",
    "           \"JG\": JordanGame,\n",
    "          \"RPS\": RockPaperScissors}\n",
    "\n",
    "run_name = time.strftime('NSP_%Y-%m-%d %a %H:%M:%S')\n",
    "game_name = setting[1]\n",
    "logdir = os.path.join(root_path, 'notebooks', 'matrix/NN', game_name, run_name)\n",
    "logdir\n",
    "\n",
    "## Experiment setup\n",
    "epoch = 10000\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 2**10\n",
    "input_length = 1\n",
    "\n",
    "# optimization params\n",
    "# NN Parameters\n",
    "learning_rate = 0.03\n",
    "optimizer_hyperparams = {\n",
    "    'lr': learning_rate\n",
    "}\n",
    "sigma = 5 #ES noise parameter\n",
    "n_perturbations = 64\n",
    "\n",
    "learner_hyperparams = {\n",
    "    'sigma': sigma,\n",
    "    'population_size': n_perturbations,\n",
    "    'scale_sigma_by_model_size': False\n",
    "}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def log_hyperparams(writer):\n",
    "    writer.add_scalar('hyperparams/batch_size', batch_size)\n",
    "    writer.add_scalar('hyperparams/learning_rate', learning_rate)\n",
    "    writer.add_scalar('hyperparams/sigma', sigma)\n",
    "    writer.add_scalar('hyperparams/n_perturbations', n_perturbations)    \n",
    "\n",
    "\n",
    "game = options[setting[1]]()\n",
    "\n",
    "# Wrapper transforming a strategy to bidder, used by the optimizer\n",
    "# this is a dummy, valuation doesn't matter\n",
    "def strat_to_player(strategy, batch_size, player_position=None):\n",
    "    return MatrixGamePlayer(strategy, batch_size = batch_size,  player_position=player_position)\n",
    "\n",
    "strats = [None] * game.n_players\n",
    "strats_copies = [None] * game.n_players\n",
    "players = [None] * game.n_players\n",
    "hist_utility = [0] * game.n_players\n",
    "hist_probs = [0] * game.n_players\n",
    "for i in range(game.n_players):\n",
    "    strats[i] = MatrixGameStrategy(n_actions=game.outcomes.shape[i],\n",
    "                                   init_weights = initial_beliefs[i],\n",
    "                                   init_weight_normalization = weight_normalization).cuda()\n",
    "\n",
    "env = MatrixGameEnvironment(game, agents=[deepcopy(a) for a in strats],\n",
    "                 n_players=game.n_players,\n",
    "                 batch_size=batch_size,\n",
    "                 strategy_to_player_closure=strat_to_player\n",
    "                 )\n",
    "\n",
    "for i in range(game.n_players):\n",
    "    players[i] = ES(model=strats[i], environment = env, hyperparams= learner_hyperparams,\n",
    "                    optimizer_type= torch.optim.SGD, optimizer_hyperparams= optimizer_hyperparams,    \n",
    "                    strat_to_player_kwargs={'player_position':i})\n",
    "    print(strats[i].distribution.probs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with SummaryWriter(log_dir=logdir) as writer:\n",
    "    torch.cuda.empty_cache()\n",
    "    log_hyperparams(writer)\n",
    "\n",
    "    for e in range(epoch+1):    \n",
    "        # always: do optimizer step\n",
    "        utility = [None] * game.n_players\n",
    "        for i in range(game.n_players):\n",
    "            utility[i] = -players[i].update_strategy_and_evaluate_utility()\n",
    "        \n",
    "        env.agents = [env._strategy_to_player(agent, batch_size, player_position) if isinstance(agent, Strategy) else agent\n",
    "            for player_position, agent in enumerate([deepcopy(a) for a in strats])]\n",
    "        \n",
    "        for i in range(game.n_players):\n",
    "            hist_utility[i] = (e * hist_utility[i] + utility[i])/ (e+1) \n",
    "            hist_probs[i] = (e * hist_probs[i] + strats[i].distribution.probs)/ (e+1)\n",
    "\n",
    "        # Logging\n",
    "        for i,strat in enumerate(strats):\n",
    "            # Historical probability for actions\n",
    "            writer.add_histogram('eval/p{}_action_distribution'.format(i), env.agents[i].get_action().view(-1).cpu().numpy(), e)\n",
    "            for a in range(len(strat.distribution.probs)-1):\n",
    "                # Historical probability for actions\n",
    "                writer.add_scalar('eval_player_{}/hist_prob_action_{}'.format(i,a), hist_probs[i][a], e)\n",
    "                # Current period actions \n",
    "                writer.add_scalar('eval_player_{}/prob_action_{}'.format(i,a), strat.distribution.probs[a], e)\n",
    "                # Expected Utility\n",
    "                writer.add_scalar('eval_player_{}/utility'.format(i), utility[i], e)\n",
    "                # Expected Historical Utility\n",
    "                writer.add_scalar('eval_player_{}/hist_utility'.format(i), hist_utility[i], e)\n",
    "        if not e % 50: print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
