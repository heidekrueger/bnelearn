{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n Player FPSB Auction with uniform symmetric valuation distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, warnings\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as ut\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from bnelearn.strategy import NeuralNetStrategy, ClosureStrategy\n",
    "from bnelearn.bidder import Bidder\n",
    "from bnelearn.mechanism import FirstPriceSealedBidAuction, VickreyAuction\n",
    "from bnelearn.learner import ESPGLearner\n",
    "from bnelearn.environment import AuctionEnvironment\n",
    "from bnelearn.experiment import Experiment\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "manual_seed = False\n",
    "if manual_seed:\n",
    "    torch.random.manual_seed(manual_seed)\n",
    "    torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Use specific cuda gpu if desired (i.e. for running multiple experiments in parallel)\n",
    "specific_gpu = 3\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "print('device', device)\n",
    "print('\\tcpu-seed', torch.random.initial_seed())\n",
    "if cuda: print('specific gpu:', torch.cuda.current_device())\n",
    "if cuda: print('\\tgpu-seed', torch.cuda.initial_seed())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in notebook folder\n",
    "# alternative for shared access to experiments:\n",
    "# log_root = os.path.abspath('/srv/bnelearn-experiments/')\n",
    "log_root = os.path.abspath('.')\n",
    "run_comment = '' # used in log title in addition to datetime\n",
    "save_figure_data_to_disc = True\n",
    "\n",
    "### Experiment setup\n",
    "epoch = 5000\n",
    "n_players = 2\n",
    "n_items = 1\n",
    "\n",
    "# valuation distribution\n",
    "u_lo =0\n",
    "u_hi =10\n",
    "\n",
    "risk = 1 # risk parameter for agent <-- not implemented in bidder yet but used in calculation of optimal utility\n",
    "\n",
    "### Environment settings\n",
    "batch_size = 2**17\n",
    "eval_batch_size = 2**25\n",
    "\n",
    "### strategy model architecture\n",
    "input_length = 1\n",
    "hidden_nodes = [5, 5]\n",
    "hidden_activations = [nn.SELU(), nn.SELU()]\n",
    "\n",
    "### Learner Settings\n",
    "learner_hyperparams = {\n",
    "    'population_size': 128,\n",
    "    'sigma': 1.,\n",
    "    'scale_sigma_by_model_size': True\n",
    "}\n",
    "\n",
    "### Optimizer Settings\n",
    "# SGD standards\n",
    "    #'lr': 1e-3,\n",
    "    #'momentum': 0.7\n",
    "# Adam standards:\n",
    "    # 'lr': 1e-3\n",
    "    # 'betas': (0.9, 0.999), #coefficients for running avgs of grad and square grad\n",
    "    # 'eps': 1e-8 , # added to denominator for numeric stability\n",
    "    # 'weight_decay': 0, #L2-decay\n",
    "    # 'amsgrad': False #whether to use amsgrad-variant\n",
    "optimizer_type = torch.optim.Adam\n",
    "optimizer_hyperparams ={    \n",
    "    'lr': 3e-3\n",
    "}\n",
    "\n",
    "### Plot and log training options\n",
    "plot_epoch = 100\n",
    "plot_points = min(100, batch_size)\n",
    "\n",
    "plot_xmin = u_lo\n",
    "plot_xmax = u_hi\n",
    "plot_ymin = 0\n",
    "plot_ymax = 10\n",
    "\n",
    "\n",
    "def strat_to_bidder(strategy, batch_size=batch_size, player_position=None, cache_actions=False):\n",
    "    return Bidder.uniform(u_lo, u_hi, strategy,\n",
    "                          batch_size = batch_size,\n",
    "                          player_position=player_position,\n",
    "                          cache_actions=cache_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_fpsb(self):\n",
    "    self.mechanism = FirstPriceSealedBidAuction(cuda = True)\n",
    "\n",
    "def setup_fpsb_bidders(self, model_sharing = True, pretrain_iters = 0):\n",
    "    if model_sharing:    \n",
    "        self.model = NeuralNetStrategy(\n",
    "            input_length, hidden_nodes = hidden_nodes,hidden_activations = hidden_activations,\n",
    "            ensure_positive_output = torch.tensor([float(u_hi)])\n",
    "        ).to(device)\n",
    "        if pretrain_iters > 0:\n",
    "            self.model.pretrain(bidders[0].valuations, pretrain_iters)\n",
    "\n",
    "        self.bidders = [strat_to_bidder(self.model, batch_size, player_position)\n",
    "                   for player_position in range(n_players)]\n",
    "    else: raise NotImplementedError(\"only model sharing has been implemented.\")\n",
    "        \n",
    "def setup_auction_environment(self):\n",
    "    self.env = AuctionEnvironment(self.mechanism, agents = self.bidders,\n",
    "                                  batch_size = batch_size, n_players =n_players,\n",
    "                                  strategy_to_player_closure = strat_to_bidder)\n",
    "def setup_espg_learner(self):\n",
    "    self.learner = ESPGLearner(\n",
    "        model = self.model, environment = self.env, hyperparams = learner_hyperparams,\n",
    "        optimizer_type = optimizer_type, optimizer_hyperparams = optimizer_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Evaluation and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the symmetric $\\sim U (\\ \\underline v, \\overline v \\ )$ setting with risk parameter $r$ and $n$ players, the bne-optimal bid is given by (Cox et al 1982)\n",
    "\n",
    "$$b^*(v) = \\underline v + \\frac{n - 1}{n-1+r} (v - \\underline v) $$\n",
    "\n",
    "The expected utility in the bne can then be calculated using\n",
    "\n",
    "$$E[u_{BNE}] = \\int_{\\underline v}^{\\overline v}{P(win | b) * u(b,v | win) *pdf(v) dv}$$\n",
    "\n",
    "In this setting, we have:\n",
    "\n",
    "$P(win | b) = P(b_i > b_j, \\forall j\\neq i) = P(b_i > b_j)^{n-1} = {\\frac{v - \\underline v}{\\overline v - \\underline v}}^{n-1}$,\n",
    "\n",
    "$u(b,v | win) = v - b^*(v) = \\frac{r}{n-1+r}$, where we use monotonicity and symmetry (i.e. $v_i \\geq v_j \\iff b_i \\geq b_j$\n",
    "\n",
    "$pdf(v) = \\frac{1}{\\overline v - \\underline v} $\n",
    "\n",
    "The integral above then works out to\n",
    "$$E(u_{BNE}) = \\frac{r(\\overline v - \\underline v)}{(n-1+r)(n+1)}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "def optimal_bid(valuation):\n",
    "    return valuation * (n_players - 1) / n_players\n",
    "\n",
    "#calculate analytical bne_utility\n",
    "bneStrategy = ClosureStrategy(optimal_bid)\n",
    "\n",
    "def setup_bne_environment(self):\n",
    "    # environment filled with optimal players for logging\n",
    "    # use higher batch size for calculating optimum\n",
    "    self.bne_env = AuctionEnvironment(self.mechanism,\n",
    "                                agents = [strat_to_bidder(bneStrategy,\n",
    "                                                          player_position= i,\n",
    "                                                          batch_size = eval_batch_size,\n",
    "                                                          cache_actions=True)\n",
    "                                          for i in range(n_players)],\n",
    "                                batch_size = eval_batch_size,\n",
    "                                n_players=n_players,\n",
    "                                strategy_to_player_closure = strat_to_bidder\n",
    "                               )\n",
    "    self.bne_utility = risk/(n_players - 1 + risk)*(u_hi - u_lo)/(n_players+1)\n",
    "\n",
    "    \n",
    "def plot_bid_function(self, fig, plot_data, writer=None, e=None, plot_points=plot_points,\n",
    "                      save_vectors_to_disc=save_figure_data_to_disc):    \n",
    "    v,b = plot_data    \n",
    "    v = v.detach().cpu().numpy()[:plot_points]\n",
    "    b= b.detach().cpu().numpy()[:plot_points]\n",
    "    \n",
    "    if save_vectors_to_disc:\n",
    "        np.savez(os.path.join(self.logdir, 'figure_data.npz'),\n",
    "                 v_opt = v_opt,b_opt = b_opt, v = v, b = b)   \n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    plt.cla()\n",
    "    plt.xlim(plot_xmin, plot_xmax)\n",
    "    plt.ylim(plot_ymin, plot_ymax)\n",
    "    plt.plot(v,b, 'o', v_opt, b_opt, 'r--')\n",
    "    #if is_ipython:\n",
    "        #display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    if writer:\n",
    "        writer.add_figure('eval/bid_function', fig, e)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_once(self, writer, e):\n",
    "    \"\"\"Everything that should be logged only once on initialization.\"\"\"\n",
    "    writer.add_scalar('debug/total_model_parameters', n_parameters, e)\n",
    "    writer.add_text('hyperparams/neural_net_spec', str(self.model), 0)    \n",
    "    writer.add_scalar('debug/eval_batch_size', eval_batch_size, e)\n",
    "    writer.add_graph(self.model, self.env.agents[0].valuations)    \n",
    "    \n",
    "def log_metrics(self, writer, e):\n",
    "    writer.add_scalar('eval/utility', self.utility, e)\n",
    "    writer.add_scalar('debug/norm_parameter_update', self.update_norm, e)\n",
    "    writer.add_scalar('eval/utility_vs_bne', self.utility_vs_bne, e)\n",
    "    writer.add_scalar('eval/epsilon_relative', self.epsilon_relative, e)\n",
    "    writer.add_scalar('debug/epsilon_absolute', self.epsilon_absolute, e) # debug because only interesting to see if numeric precision is a problem, otherwise same as relative but scaled.\n",
    "\n",
    "# TODO: deferred until writing logger\n",
    "def log_hyperparams(self, writer, e):\n",
    "    \"\"\"Everything that should be logged on every learning_rate updates\"\"\"\n",
    "#     writer.add_scalar('hyperparams/batch_size', batch_size, e)\n",
    "#     writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "#     writer.add_scalar('hyperparams/momentum', momentum, e)\n",
    "#     writer.add_scalar('hyperparams/sigma', sigma, e)\n",
    "#     writer.add_scalar('hyperparams/n_perturbations', n_perturbations, e)\n",
    "\n",
    "def training_loop(self, writer, e):    \n",
    "\n",
    "    ### do in every iteration ###\n",
    "    # save current params to calculate update norm\n",
    "    prev_params = torch.nn.utils.parameters_to_vector(self.model.parameters())\n",
    "    #update model\n",
    "    self.utility = self.learner.update_strategy_and_evaluate_utility()\n",
    "    \n",
    "    ## everything after this is logging --> measure overhead\n",
    "    start_time = timer()\n",
    "    \n",
    "    # calculate infinity-norm of update step\n",
    "    new_params = torch.nn.utils.parameters_to_vector(self.model.parameters())\n",
    "    self.update_norm = (new_params - prev_params).norm(float('inf'))    \n",
    "    # calculate utility vs bne    \n",
    "    self.utility_vs_bne = self.bne_env.get_reward(strat_to_bidder(self.model, batch_size = eval_batch_size), draw_valuations=False)\n",
    "    self.epsilon_relative = 1 - self.utility_vs_bne / self.bne_utility\n",
    "    self.epsilon_absolute = self.bne_utility - self.utility_vs_bne\n",
    "    \n",
    "    self.log_metrics(writer, e)\n",
    "    \n",
    "    if e % plot_epoch == 0:\n",
    "        # plot current function output\n",
    "        #bidder = strat_to_bidder(model, batch_size)\n",
    "        #bidder.draw_valuations_()\n",
    "        v = self.bidders[0].valuations\n",
    "        b = self.bidders[0].get_action()\n",
    "        plot_data = (v,b)\n",
    "\n",
    "        print(\"Epoch {}: \\tcurrent utility: {:.3f},\\t utility vs BNE: {:.3f}, \\tepsilon (abs/rel): ({:.5f}, {:.5f})\".format(\n",
    "            e, self.utility, self.utility_vs_bne, self.epsilon_absolute, self.epsilon_relative))\n",
    "        self.plot(self.fig, plot_data ,writer,e)\n",
    "            \n",
    "    elapsed = timer() - start_time        \n",
    "    self.overhead_mins = self.overhead_mins + elapsed/60\n",
    "    writer.add_scalar('debug/overhead_mins', self.overhead_mins, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricFPSBExperiment(Experiment):\n",
    "    setup_game = setup_fpsb\n",
    "    setup_players = setup_fpsb_bidders\n",
    "    setup_learning_environment = setup_auction_environment\n",
    "    setup_learners = setup_espg_learner\n",
    "    equilibrium_strategy = optimal_bid\n",
    "    setup_eval_environment = setup_bne_environment\n",
    "    plot = plot_bid_function\n",
    "    log_once = log_once\n",
    "    log_metrics = log_metrics\n",
    "    log_hyperparams = log_hyperparams\n",
    "    training_loop = training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = SymmetricFPSBExperiment(\n",
    "    name = ['single_item', 'fpsb', 'uniform', 'symmetric', str(n_players)+'p'],\n",
    "    options = None, device = device, specific_gpu = specific_gpu, seed = None, log_root = log_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp.model)\n",
    "n_parameters = sum([p.numel() for p in exp.model.parameters()])\n",
    "print('Total parameters: ' + str(n_parameters))\n",
    "\n",
    "## Check Setup\n",
    "\n",
    "# when calculating utilities, make sure valuations are drawn at least once.\n",
    "print(\"Utility in BNE (analytical): \\t{:.5f}\".format(exp.bne_utility))\n",
    "bne_utility_sampled = exp.bne_env.get_reward(exp.bne_env.agents[0], draw_valuations=True)\n",
    "print('Utility in BNE (sampled): \\t{:.5f}'.format(bne_utility_sampled))\n",
    "utility_vs_bne = exp.bne_env.get_strategy_reward(exp.model, player_position=0)\n",
    "print('Model utility vs BNE: \\t\\t{:.5f}'.format(utility_vs_bne))\n",
    "utility_learning_env = exp.env.get_strategy_reward(exp.model, player_position=0, draw_valuations = True)\n",
    "print('Model utility in learning env:\\t{:.5f}'.format(utility_learning_env))\n",
    "\n",
    "v_opt = np.linspace(plot_xmin, plot_xmax, 100)\n",
    "b_opt = optimal_bid(v_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run(epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
