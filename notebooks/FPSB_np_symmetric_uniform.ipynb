{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n Player FPSB Auction with uniform symmetric valuation distributions\n",
    "\n",
    "## _symmetric_ implementation with _dynamic_ environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as ut\n",
    "from torch.optim.optimizer import Optimizer, required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnelearn.strategy import NeuralNetStrategy, TruthfulStrategy\n",
    "from bnelearn.bidder import Bidder\n",
    "from bnelearn.mechanism import FirstPriceSealedBidAuction, VickreyAuction\n",
    "from bnelearn.optimizer import ES\n",
    "from bnelearn.environment import AuctionEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "#\n",
    "#plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "# Use specific cuda gpu if desired (i.e. for running multiple experiments in parallel)\n",
    "specific_gpu = 7\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The following cell fully defines an experiment.\n",
    "The following set of parameters works well for 2p:\n",
    "```\n",
    "batch_size = 2**14\n",
    "size_hidden_layer = 10\n",
    "\n",
    "learning_rate = 1e-1\n",
    "lr_decay = True\n",
    "lr_decay_every = 500\n",
    "lr_decay_factor = 0.8\n",
    "baseline = True\n",
    "momentum = 0.7\n",
    "\n",
    "sigma = .02 #ES noise parameter\n",
    "n_perturbations = 256\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = 'fpsb/10p/uniform/symmetric'\n",
    "run_name = '3'\n",
    "logdir = os.path.join(root_path, 'notebooks', run_dir , run_name)\n",
    "\n",
    "## Experiment setup\n",
    "n_players = 10\n",
    "n_items = 1\n",
    "# valuation distribution\n",
    "u_lo =0\n",
    "u_hi =10\n",
    "\n",
    "def strat_to_bidder(strategy, batch_size):\n",
    "    return Bidder.uniform(u_lo, u_hi, strategy, batch_size = batch_size, n_players=1)\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 2**17\n",
    "input_length = 1\n",
    "\n",
    "# strategy model architecture\n",
    "size_hidden_layer = 10\n",
    "\n",
    "# optimization params\n",
    "epoch = 10000\n",
    "learning_rate = 3e-1\n",
    "lr_decay = True\n",
    "lr_decay_every = 1000\n",
    "lr_decay_factor = 0.75\n",
    "baseline = True\n",
    "momentum = 0.7\n",
    "\n",
    "sigma = .05 #ES noise parameter\n",
    "n_perturbations = 128\n",
    "\n",
    "# plot and log training options\n",
    "plot_epoch = 250\n",
    "plot_points = min(100, batch_size)\n",
    "\n",
    "plot_xmin = u_lo\n",
    "plot_xmax = u_hi\n",
    "plot_ymin = 0\n",
    "plot_ymax = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "def optimal_bid(valuation):\n",
    "    return valuation * (n_players - 1) / n_players\n",
    "\n",
    "def log_hyperparams(writer, e):\n",
    "    writer.add_scalar('hyperparams/batch_size', batch_size, e)\n",
    "    writer.add_scalar('hyperparams/size_hidden_layer', size_hidden_layer, 0)\n",
    "    writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "    writer.add_scalar('hyperparams/momentum', momentum, e)\n",
    "    writer.add_scalar('hyperparams/sigma', sigma, e)\n",
    "    writer.add_scalar('hyperparams/n_perturbations', n_perturbations, e)\n",
    "\n",
    "v_opt = np.linspace(plot_xmin, plot_xmax, 100)\n",
    "b_opt = optimal_bid(v_opt)\n",
    "    \n",
    "def plot_bid_function(fig, v,b, writer=None, e=None, plot_points=plot_points):\n",
    "    \n",
    "    # subsample points and plot\n",
    "    v = v.detach().cpu().numpy()[:plot_points]\n",
    "    b= b.detach().cpu().numpy()[:plot_points]\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    plt.cla()\n",
    "    plt.xlim(plot_xmin, plot_xmax)\n",
    "    plt.ylim(plot_ymin, plot_ymax)\n",
    "    plt.plot(v,b, 'o', v_opt, b_opt, 'r--')\n",
    "    #if is_ipython:\n",
    "        #display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    if writer:\n",
    "        writer.add_figure('eval/bid_function', fig, e)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model.\n",
    "We'll ensure the initialization provides positive outputs on the domain we are interested in, as otherwise we can't learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_is_positive = False\n",
    "while not output_is_positive:\n",
    "    model = NeuralNetStrategy(input_length,\n",
    "                              size_hidden_layer = size_hidden_layer,\n",
    "                              requires_grad=False\n",
    "                             ).to(device)\n",
    "    \n",
    "    if model(torch.tensor([float(u_hi)], device=device)) > 0:\n",
    "        output_is_positive = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism = FirstPriceSealedBidAuction(cuda = True)\n",
    "env = AuctionEnvironment(mechanism,\n",
    "                  agents = [], #dynamically built\n",
    "                  max_env_size = n_players - 1, # n-1 opponents\n",
    "                  batch_size = batch_size,\n",
    "                  n_players =n_players,\n",
    "                  strategy_to_bidder_closure = strat_to_bidder\n",
    "                 )\n",
    "optimizer = ES(model=model, environment = env,\n",
    "               lr = learning_rate, momentum=momentum,\n",
    "               sigma=sigma, n_perturbations=n_perturbations,\n",
    "               baseline=baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SummaryWriter(logdir, flush_secs=30) as writer:\n",
    "    overhead_mins = 0\n",
    "    torch.cuda.empty_cache()\n",
    "    log_hyperparams(writer, 0)\n",
    "    fig = plt.figure()\n",
    "    for e in range(epoch+1):\n",
    "        # lr decay?\n",
    "        if lr_decay and e % lr_decay_every == 0 and e > 0:\n",
    "            learning_rate = learning_rate * lr_decay_factor\n",
    "            log_hyperparams(writer, e)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "\n",
    "        # always: do optimizer step\n",
    "        utility = -optimizer.step()\n",
    "        writer.add_scalar('eval/utility', utility, e)\n",
    "\n",
    "        if e % plot_epoch == 0:\n",
    "            start_time = timer()\n",
    "            # plot current function output\n",
    "            bidder = strat_to_bidder(model, batch_size)\n",
    "            bidder.draw_valuations_()\n",
    "            v = bidder.valuations\n",
    "            b = bidder.get_action()\n",
    "            share = b.mean()/optimal_bid(v).mean()\n",
    "            diff = (b-optimal_bid(v)).mean()\n",
    "            writer.add_scalar('eval/share', share, e)\n",
    "            writer.add_scalar('eval/diff', diff, e)\n",
    "            writer.add_graph(model, bidder.valuations) \n",
    "\n",
    "            print(\"Epoch {}: \\ttotal share: {:.3f}, diff: {:.3f}, \\tutility: {:.3f}\".format(e, share, diff, utility))\n",
    "            plot_bid_function(fig, v,b,writer,e)\n",
    "            \n",
    "            elapsed = timer() - start_time\n",
    "            overhead_mins = overhead_mins + elapsed/60\n",
    "            writer.add_scalar('eval/overhead_mins', overhead_mins, e)\n",
    "            \n",
    "            print(\"Logging checkpoint took {:.2f}s.\".format(elapsed))         \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
