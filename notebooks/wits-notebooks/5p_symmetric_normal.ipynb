{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n Player FPSB Auction with symmetric valuation distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, warnings\n",
    "root_path = os.path.abspath(os.path.join('..'))\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as ut\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from bnelearn.strategy import NeuralNetStrategy, ClosureStrategy\n",
    "from bnelearn.bidder import Bidder\n",
    "from bnelearn.mechanism import FirstPriceSealedBidAuction\n",
    "from bnelearn.optimizer import ES\n",
    "from bnelearn.environment import AuctionEnvironment\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "# larger plots\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "    \n",
    "cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "# Use specific cuda gpu if desired \n",
    "#(i.e. for running multiple experiments in parallel)\n",
    "specific_gpu = 2\n",
    "if cuda and specific_gpu:\n",
    "    torch.cuda.set_device(specific_gpu)\n",
    "\n",
    "print(device)\n",
    "if cuda: print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "The following cell fully defines an experiment.\n",
    "\n",
    "The folowing settings are known to work well:\n",
    "```\n",
    "3p:  batch_size = 2**17, learning_rate = 5e-1, lr_decay_every = 500, lr_decay_factor = 0.7,\n",
    "     momentum = 0.5, sigma = .02, n_perturbations = 128,\n",
    "     eval_batch_size 2**15\n",
    "     hidden_nodes = [5,5], hidden_activations = [nn.SELU(), nn.SELU()]\n",
    "     \n",
    "5p: (run 2019-07-18 Thu 22:18)\n",
    "    Results: [# epochs ~ relative epsilon @  smoothing level (chosen by eye]\n",
    "         1k ~ 9e-3 @ .9\n",
    "         5k ~ 1e-3 @ .99\n",
    "        10k ~ 5e-4 @ .995\n",
    "        14k ~ 1e-4 @ .995\n",
    "        \n",
    "    Architecture:\n",
    "        hidden_nodes = [10, 10]\n",
    "        hidden_activations = [nn.SELU(), nn.SELU()]\n",
    "    Hparams:\n",
    "        batch_size = 2**17, eval_batch_size 2**15\n",
    "        learning_rate = 5e-1, lr_decay_every = 500, lr_decay_factor = 0.8,\n",
    "        momentum = 0.8, sigma = .05, n_perturbations = 256\n",
    "        \n",
    "10p:(run 2019-07-24 Wed 11:39)\n",
    "    Results: [# epochs ~ relative epsilon @  smoothing level (chosen by eye)]\n",
    "         1k ~ 1e-1 @ .9\n",
    "         5k ~ 3e-2 @ .95\n",
    "        10k ~ 4e-3 @ .95\n",
    "        15k ~ 4e-3 @ .99\n",
    "        20k ~ 1e-3 @ .99\n",
    "    average absolute errors are on the order of 5e-4\n",
    "    finds exact BNE except for b <~2.5 where it underbids   \n",
    "        \n",
    "    Architecture:\n",
    "        hidden_nodes = [10, 10]\n",
    "        hidden_activations = [nn.SELU(), nn.SELU()]\n",
    "    Hparams:\n",
    "        batch_size = 2**17, eval_batch_size 2**15\n",
    "        learning_rate = 4e-1, lr_decay_every = 2000, lr_decay_factor = 0.8,\n",
    "        momentum = 0.7, sigma = .02, n_perturbations = 64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in notebook folder\n",
    "# alternative for shared access to experiments:\n",
    "log_root = os.path.abspath('/srv/bnelearn/wits-experiments')\n",
    "#log_root = os.path.abspath('.')\n",
    "run_comment = '' # used in log title in addition to datetime\n",
    "save_figure_data_to_disc = True\n",
    "\n",
    "## Experiment setup\n",
    "n_players = 5\n",
    "n_items = 1\n",
    "\n",
    "# valuation distribution\n",
    "valuation_mean = 10.0\n",
    "valuation_std = 5.0\n",
    "\n",
    "## Environment settings\n",
    "#training batch size\n",
    "batch_size = 2**17\n",
    "input_length = 1\n",
    "\n",
    "eval_batch_size = 2**20\n",
    "n_processes_optimal_strategy = 24\n",
    "\n",
    "# strategy model architecture\n",
    "hidden_nodes = [10, 10]\n",
    "hidden_activations = [nn.SELU(), nn.SELU()]\n",
    "\n",
    "# optimization params\n",
    "epoch = 10000\n",
    "learning_rate = 5e-1\n",
    "lr_decay = True\n",
    "lr_decay_every = 500\n",
    "lr_decay_factor = 0.8\n",
    "momentum = 0.8\n",
    "\n",
    "sigma = .03 #ES noise parameter\n",
    "n_perturbations = 128\n",
    "\n",
    "# plot and log training options\n",
    "plot_epoch = 1000 #plot and log optima this often\n",
    "write_graph = True # whether to log graph to disk\n",
    "plot_points = min(150, batch_size)\n",
    "plot_xmin = int(max(0, valuation_mean - 3*valuation_std))\n",
    "plot_xmax = int(valuation_mean + 3*valuation_std)\n",
    "\n",
    "plot_ymin = 0\n",
    "plot_ymax = 20\n",
    "\n",
    "def strat_to_bidder(strategy, batch_size=batch_size, player_position=None, cache_actions=False):\n",
    "    return Bidder.normal(valuation_mean,valuation_std, strategy,\n",
    "                         batch_size = batch_size,\n",
    "                         player_position=player_position,\n",
    "                         cache_actions=cache_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism = FirstPriceSealedBidAuction(cuda = True)\n",
    "\n",
    "model = NeuralNetStrategy(input_length,\n",
    "                          hidden_nodes = hidden_nodes,\n",
    "                          hidden_activations = hidden_activations,\n",
    "                          requires_grad=False,\n",
    "                          ensure_positive_output = torch.tensor([float(valuation_mean)])\n",
    "                          ).to(device) \n",
    "\n",
    "bidders = [strat_to_bidder(model, batch_size, player_position)\n",
    "           for player_position in range(n_players)]\n",
    "\n",
    "env = AuctionEnvironment(mechanism,\n",
    "                  agents = bidders,\n",
    "                  batch_size = batch_size,\n",
    "                  n_players =n_players,\n",
    "                  strategy_to_player_closure = strat_to_bidder\n",
    "                 )\n",
    "optimizer = ES(model=model, environment = env,\n",
    "               lr = learning_rate, momentum=momentum,\n",
    "               sigma=sigma, n_perturbations=n_perturbations)\n",
    "\n",
    "print(model)\n",
    "n_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print('Total parameters: ' + str(n_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Evaluation and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Menezes et al. 2005., the optimal bid for symmetric valuations $v$ that are distributed with pdf $f(v)$ and cdf $F(v)$ for $n$ players in this setting is given by\n",
    "\n",
    "$$b^*(v) = v - \\frac{\\int_0^v F(x)^{n-1} dx}{F(v)^{n-1}} $$\n",
    "\n",
    "We implement it here for calculating comparison metrics.\n",
    "\n",
    "The expected utility in the bne can then be calculated using\n",
    "\n",
    "$$E[u_{BNE}] = \\int_{0}^{\\infty}{P(win | b) * u(b,v | win) *f(v) dv}$$\n",
    "\n",
    "In this setting, we have:\n",
    "\n",
    "$P(win | b) = P(b_i > b_j, \\forall j\\neq i) = P(b_i > b_j)^{n-1} = {F(v)}^{n-1}$, where we use monotonicity and symmetry (i.e. $v_i \\geq v_j \\iff b_i \\geq b_j$\n",
    "\n",
    "$u(b,v | win) = v - b^*(v) = \\frac{\\int_0^v F(x)^{n-1} dx}{F(v)^{n-1}}$, \n",
    "\n",
    "$f(v)$ directly given by arbitrary distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integral above then works out to\n",
    "$$E(u_{BNE}) = \\int_{0}^{\\infty}\\int_{0}^{v}F(x)^{n-1}dx\\ f(v) dv $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "common_dist = torch.distributions.normal.Normal(loc = valuation_mean, scale = valuation_std)\n",
    "\n",
    "# TODO: investigate where everything is allocated. possibly move GPU vectors to CPU completely instead of shuffling around for integration?\n",
    "def optimal_bid(valuation: torch.Tensor or np.ndarray or float) -> torch.Tensor:\n",
    "    \n",
    "    # For float and numpy --> convert to tensor\n",
    "    if not isinstance(valuation, torch.Tensor):\n",
    "        valuation = torch.tensor(valuation, dtype = torch.float)           \n",
    "    # For float / 0d tensors --> unsqueeze to allow list comprehension below\n",
    "    if valuation.dim() == 0:\n",
    "        valuation.unsqueeze_(0)\n",
    "    \n",
    "    # shorthand notation for F^(n-1)\n",
    "    Fpowered = lambda v: torch.pow(common_dist.cdf(v), n_players - 1)  \n",
    "    \n",
    "    # do the calculations\n",
    "    numerator = torch.tensor(\n",
    "            [integrate.quad(Fpowered, 0, v)[0] for v in valuation],\n",
    "            device = valuation.device\n",
    "        ).reshape(valuation.shape)                                 \n",
    "    return valuation - numerator / Fpowered(valuation)\n",
    "\n",
    "# equilibrium Strategy and environment in equilibirum\n",
    "bneStrategy = ClosureStrategy(optimal_bid, parallel=n_processes_optimal_strategy)\n",
    "# set up an environment populated by players that play the optimum\n",
    "bne_env = AuctionEnvironment(\n",
    "    mechanism,\n",
    "    agents = [strat_to_bidder(bneStrategy,\n",
    "                              player_position= i,\n",
    "                              batch_size = eval_batch_size,\n",
    "                              cache_actions = True) \n",
    "              for i in range(n_players)\n",
    "             ],\n",
    "    batch_size = eval_batch_size,\n",
    "    n_players=n_players,\n",
    "    strategy_to_player_closure = strat_to_bidder)\n",
    "\n",
    "# calculate analytical and sampled bne-utilities\n",
    "with warnings.catch_warnings(): \n",
    "    warnings.simplefilter('ignore')\n",
    "    # don't print scipy accuracy warnings\n",
    "    bne_utility, analytical_error = integrate.dblquad(\n",
    "        lambda x,v: common_dist.cdf(x)**(n_players - 1) * common_dist.log_prob(v).exp(),\n",
    "        0, float('inf'), # outer boundaries\n",
    "        lambda v: 0, lambda v: v) # inner boundaries\n",
    "    bne_utility_sampled = bne_env.get_reward(bne_env.agents[0], draw_valuations = True)\n",
    "\n",
    "if analytical_error > 1e-7:\n",
    "    warnings.warn('Error in optimal utility might not be negligible')\n",
    "\n",
    "print(\"Utility in BNE (analytical): \\t{:.5f}\".format(bne_utility))\n",
    "print('Utility in BNE (sampled): \\t{:.5f}'.format(bne_utility_sampled))\n",
    "utility_vs_bne = bne_env.get_strategy_reward(model, player_position=0)\n",
    "print('Model utility vs BNE: \\t\\t{:.5f}'.format(utility_vs_bne))\n",
    "utility_learning_env = env.get_strategy_reward(model, player_position=0, draw_valuations = True)\n",
    "print('Model utility in learning env:\\t{:.5f}'.format(utility_learning_env))\n",
    "  \n",
    "    \n",
    "# predefine points for plotting optimal curve to save cpu-bound integrations\n",
    "v_opt = np.linspace(plot_xmin, plot_xmax, 100) # 100 points more than enough\n",
    "b_opt = optimal_bid(v_opt).numpy()\n",
    "\n",
    "def plot_bid_function(fig, v,b, writer=None, e=None, plot_points=plot_points,\n",
    "                      save_vectors_to_disc=save_figure_data_to_disc):\n",
    "    \n",
    "    # subsample points and plot\n",
    "    v = v.detach().cpu().numpy()[:plot_points]\n",
    "    b= b.detach().cpu().numpy()[:plot_points]\n",
    "    \n",
    "    if save_vectors_to_disc:\n",
    "        np.savez(\n",
    "            os.path.join(logdir, 'figure_data.npz'),\n",
    "            v_opt = v_opt,\n",
    "            b_opt = b_opt,\n",
    "            v = v, b = b\n",
    "        )\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    plt.cla()\n",
    "    plt.xlim(plot_xmin, plot_xmax)\n",
    "    plt.ylim(plot_ymin, plot_ymax)\n",
    "    plt.plot(v,b, 'o', v_opt, b_opt, 'r--')\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    if writer:\n",
    "        writer.add_figure('eval/bid_function', fig, e)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot optimal bid to ensure appropriate boundaries have been chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_once(writer, e):\n",
    "    \"\"\"Everything that should be logged only once on initialization.\"\"\"\n",
    "    writer.add_scalar('debug/total_model_parameters', n_parameters, e)\n",
    "    writer.add_text('hyperparams/neural_net_spec', str(model), 0)    \n",
    "    writer.add_scalar('debug/eval_batch_size', eval_batch_size, e)\n",
    "    writer.add_graph(model, env.agents[0].valuations)   \n",
    "\n",
    "def log_hyperparams(writer, e):\n",
    "    writer.add_scalar('hyperparams/batch_size', batch_size, e)\n",
    "    writer.add_scalar('hyperparams/learning_rate', learning_rate, e)\n",
    "    writer.add_scalar('hyperparams/momentum', momentum, e)\n",
    "    writer.add_scalar('hyperparams/sigma', sigma, e)\n",
    "    writer.add_scalar('hyperparams/n_perturbations', n_perturbations, e)\n",
    "    \n",
    "def log_metrics(writer, e):\n",
    "    \"Everything that is logged in every iteration.\"\n",
    "    writer.add_scalar('eval/utility', utility, e)\n",
    "    writer.add_scalar('debug/norm_parameter_update', update_norm, e)\n",
    "    writer.add_scalar('eval/utility_vs_bne', utility_vs_bne, e)\n",
    "    writer.add_scalar('eval/epsilon_relative', epsilon_relative, e)\n",
    "    writer.add_scalar('debug/epsilon_absolute', epsilon_absolute, e) # debug because only interesting to see if numeric precision is a problem, otherwise same as relative but scaled.\n",
    "\n",
    "def training_loop(e, writer):    \n",
    "    global overhead_mins, learning_rate,\\\n",
    "        utility, utility_vs_bne, epsilon_relative, epsilon_absolute, update_norm\n",
    "    \n",
    "    if lr_decay and e % lr_decay_every == 0 and e > 0:\n",
    "        learning_rate = learning_rate * lr_decay_factor\n",
    "        log_hyperparams(writer, e)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    \n",
    "    ### do in every iteration ###\n",
    "    # save current params to calculate update norm\n",
    "    prev_params = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    #update model\n",
    "    utility = -optimizer.step()\n",
    "    \n",
    "    ## everything beyond this is logging --> measure overhead    \n",
    "    start_time = timer()\n",
    "    \n",
    "    # calculate infinity-norm of update step\n",
    "    new_params = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    update_norm = (new_params - prev_params).norm(float('inf'))\n",
    "    # calculate utility vs bne    \n",
    "    utility_vs_bne = bne_env.get_reward(strat_to_bidder(model, batch_size = eval_batch_size), draw_valuations=False)\n",
    "    epsilon_relative = 1 - utility_vs_bne / bne_utility\n",
    "    epsilon_absolute = bne_utility - utility_vs_bne\n",
    "    \n",
    "    #log that ðŸ’©\n",
    "    log_metrics(writer, e)\n",
    "\n",
    "    if e % plot_epoch == 0:        \n",
    "        # plot current function output\n",
    "        v = bidders[0].valuations\n",
    "        b = bidders[0].get_action()\n",
    "        print(\"Epoch {}: \\tcurrent utility: {:.3f},\\t utility vs BNE: {:.3f}, \\tepsilon (abs/rel): ({:.5f}, {:.5f})\".format(e, utility, utility_vs_bne, epsilon_absolute, epsilon_relative))\n",
    "        plot_bid_function(fig, v,b,writer,e)\n",
    "            \n",
    "    elapsed = timer() - start_time            \n",
    "    overhead_mins = overhead_mins + elapsed/60\n",
    "    writer.add_scalar('debug/overhead_mins', overhead_mins, e)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logger\n",
    "if os.name == 'nt': raise ValueError('The run_name may not contain : on Windows! (change datetime format to fix this)') \n",
    "run_name = time.strftime('%Y-%m-%d %a %H:%M')\n",
    "if run_comment:\n",
    "    run_name = run_name + ' - ' + str(run_comment)\n",
    "logdir = os.path.join(log_root, 'fpsb', 'symmetric', 'normal', str(n_players) + 'p', run_name)\n",
    "e = 0\n",
    "overhead_mins = 0\n",
    "\n",
    "print(logdir)\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "fig = plt.figure()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with SummaryWriter(logdir, flush_secs=30) as writer:\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    log_once(writer, 0)\n",
    "    log_hyperparams(writer, 0)    \n",
    "    \n",
    "    for e in range(e,e+epoch+1):\n",
    "        training_loop(e, writer)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
